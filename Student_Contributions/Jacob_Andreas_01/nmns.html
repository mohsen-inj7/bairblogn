<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Learning to reason with neural module networks</title>
  <meta name="description" content="Suppose we’re building a household robot, and want it to be able to answerquestions about its surroundings. We might ask questions like these:">

  <link href="http://people.eecs.berkeley.edu/~jda/css/bootstrap.min.css" rel="stylesheet">
  <link href="http://people.eecs.berkeley.edu/~jda/css/style.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css" />
  <link rel="stylesheet" href="/css/solarized-light.css" />
  <link rel="canonical" href="http://localhost:4000/nmns.html">
  <link rel="alternate" type="application/rss+xml" title="blog" href="http://localhost:4000/feed.xml" />

  <script type="text/javascript"
          src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
    <div class="content">

    <div class="post content">

  <header class="post-header">
    <p class="home"><a href="/">Home</a></p>
    <p class="post-meta">May 17, 2017</p>
    <h1 class="post-title">Learning to reason with neural module networks</h1>
  </header>

  <article class="post-content">
    <p>Suppose we’re building a household robot, and want it to be able to answer
questions about its surroundings. We might ask questions like these:</p>

<p><img src="figures/nmns/examples.jpg" /></p>

<p>How can we ensure that the robot can answer these questions correctly? The
standard approach in deep learning is to collect a large dataset of questions,
images, and answers, and train a single neural network to map directly from
questions and images to answers.  If most questions look like the one on the
left, we have a familiar image recognition problem, and approaches like the one
shown above are quite effective:</p>

<p><img src="figures/nmns/cat_pred.jpg" /></p>

<p>But things don’t work quite so well for approaches like the one on the right:</p>

<p><img src="figures/nmns/clevr_pred.jpg" /></p>

<p>Here the neural net has given up and guessed the most common color in the image.
What makes this question so much harder? Even though the image is cleaner, the
question requires many steps of <em>reasoning</em>: rather than simply recognizing the
main object in the image, the model must first find the blue cylinder, locate
the other object with the same size, and then determine its color. This is a
complicated computation, and it’s a computation <em>specific to the question that
was asked</em>. Different questions require different sequences of steps to solve.</p>

<p>This stands in contrast to the dominant “one size fits all” paradigm in deep
learning: for whatever problem we’re trying to solve, we write down a fixed
model architecture that we hope captures the relationship between the input and
output, and learn parameters for that fixed model from labeled training data.</p>

<p>But real-world reasoning doesn’t work this way: it involves a variety of
different capabilities, combined and synthesized in new ways for every new 
challenge we encounter in the wild. What we need is a model that can
<em>dynamically</em> determine how to reason about the problem in front of it—a
network that can choose its own structure on the fly. In this post, we’ll talk
about a new class of models we call <strong>neural module networks</strong> (NMNs), which
incorporate this more flexible approach to problem-solving while preserving the
expressive power that makes deep learning so effective.</p>

<hr />

<p>Earlier, we noticed that there are three different steps involved in answering
the question above: finding a blue cylinder, finding something else the same
size, and determining its color. We can draw this schematically like:</p>

<p><img src="figures/nmns/layout1.jpg" /></p>

<p>A different question might involve a different series of steps. If we ask “how
many things are the same size as the ball?”, we might have something like:</p>

<p><img src="figures/nmns/layout2.jpg" /></p>

<p>Basic operations like “compare size” are shared between questions, but they get
used in different ways. The key idea behind NMNs is to make this sharing
explicit: we use two different network structures to answer the two questions
above, but we share weights between pieces of networks that involve the same
basic operations:</p>

<p><img src="figures/nmns/tying.jpg" /></p>

<p>How do we learn a model like this? Rather than training a single large network
on lots of input / ouptut pairs, we actually train a huge number of different
networks at the same time, while tying their parameters together where
appropriate:</p>

<p><img src="figures/nmns/training.jpg" /></p>

<p>(Several recent deep learning frameworks, including DyNet and TensorFlow Fold,
were explicitly designed with this kind of dynamic computation in mind.)</p>

<p>What we get at the end of the training process is not a single deep network, but
rather a collection of neural “modules”, each of which implements a single step
of reasoning. When we want to use our trained model on a new problem instance,
we can assemble these modules dynamically to produce a new network structure
tailored to that problem.</p>

<p>One of the remarkable things about this process is that we don’t need to provide
any low-level supervision for individual modules: the model never sees an
isolated example of blue object or a “left-of” relationship. Modules are learned
only inside larger composed structures, with only (question, answer) pairs as
supervision. But the training procedure is able <em>automatically</em> infer the
correct relationship between pieces of structure and the computations they’re
responsible for:</p>

<p><img src="figures/nmns/exploded.jpg" /></p>

<hr />

<p>The key ingredient in this whole process is a collection of high-level
“reasoning blueprints” like the ones above. These blueprints tell us how the
network for each question should be laid out, and how different questions relate
to one another. But where do the blueprints come from?</p>

<p>In our initial work on these models, we drew on a surprising connection
between the problem of designing question-specific neural networks and the
problem of analyzing grammatical structure. Linguists have long observed that
the grammar of a question is closely related to the sequence of computational
steps needed to answer it. Thanks to recent advances in natural language
processing, we can use off-the-shelf tools for grammatical analysis to provide
approximate version of these blueprints automatically.</p>

<p>But finding exactly the right mapping from linguistic structure to network
structure Is still a challenging problem, and the conversion process is prone
to errors. In later work, rather than relying on this kind of linguistic
analysis, we instead turned to data produced by human experts who directly
labeled a collection of questions with idealized reasoning blueprints.  By
learning to imitate these humans, our model was able to improve the quality of
its predictions substantially. Most surprisingly, when we took a model trained
to imitiate experts, but allowed it to explore its own modifications to these
expert predictions, it was able to find even better solutions than experts on a
wide variety of problems.</p>

<hr />

<p>Despite the remarkable success of deep learning methods in recent years, many
problems—including few-shot learning and complex reasoning—remain a
challenge. But these are exactly the sorts of problems where more structured
classical techniques like semantic parsing and program induction really shine.
Neural module networks give us the best of both worlds: the flexibility and data
efficiency of discrete compositionality, combined with the representational
power of deep networks. NMNs have already seen a number of successes for
visual reasoning problems, and we’re excited to start applying them to other AI
problems as well.</p>


  </article>

  
    <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'jacobandreas-blog';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  

</div>


    </div>
  </body>

</html>
