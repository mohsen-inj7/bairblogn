---
layout:             post
title:              "Unsupervised Meta-Learning: Learning to Learn without Supervision"
date:               2020-05-01 9:00:00
author:             <a href="https://ben-eysenbach.github.io/">Benjamin Eysenbach</a> and <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>
img:                assets/umrl/Unsupervised-Meta-Learning-v2.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<!--
TODO TODO TODO personal reminder for Daniel Seita :-)
Be careful that these three lines are at the top,
and that the title and image change for each blog post!
-->
<meta name="twitter:title" content="Unsupervised Meta-Learning: Learning to Learn without Supervision">
<meta name="twitter:card" content="summary_image">
<meta name="twitter:image" content="https://bair.berkeley.edu/static/blog/umrl/Unsupervised-Meta-Learning-v2.png">

*This post is cross-listed [on the CMU ML blog][1].*

The history of machine learning has largely been a story of increasing
abstraction. In the dawn of ML, researchers spent considerable effort
engineering features. As deep learning gained popularity, researchers then
shifted towards tuning the update rules and learning rates for their
optimizers. Recent research in meta-learning has climbed one level of
abstraction higher; many researchers now spend their days manually constructing
task distributions, from which they can automatically learn good optimizers.
What might be the next rung on this ladder? In this post we introduce theory
and algorithms for unsupervised meta-learning, where machine learning
algorithms themselves propose their own task distributions. Unsupervised
meta-learning further reduces the amount of human supervision required to solve
tasks, potentially providing the next rung on this ladder of abstraction.

<!--more-->

We start by discussing how machine learning algorithms use human supervision to
find patterns and extract knowledge from observed data. The most common machine
learning setting is *supervised learning*, where a dataset of input-output
pairs is provided. The aim is to return a predictor that correctly assigns
outputs for novel inputs. Another common machine learning problem setting is
*reinforcement learning (RL)*, where an agent takes actions in an environment.
In RL, humans indicate the desired behavior through a reward function, which
the agent  seeks to maximize. To draw a crude analogy to supervised learning,
the environment dynamics are the examples $X$, and the reward function gives
the labels $Y$. There are many algorithms for supervised learning and RL,
including tabular methods (e.g., value iteration), linear methods (e.g., linear
regression) and kernel-methods (e.g., RBF-SVMs). Broadly, we call these
algorithms **learning procedures**: processes that take as input a dataset
(examples with labels, or transitions with rewards) and output a function that
performs well (achieves high accuracy or large reward) on the dataset.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/umrl/control_room.jpg" width="500">
<br />
<i>
Machine learning research is similar to the control room for large physics
experiments. Researchers have a number of knobs they can tune which affect the
performance of the learning procedure. The right setting for the knobs depends
on the particular experiment: some settings work well for high-energy
experiments; others work well for ultracold atom experiments.
<a href="https://www.flickr.com/photos/x-ray_delta_one/3941701730">Figure Credit</a>.
</i>
</p>

Similar to lab procedures used in the physical sciences, the learning
procedures used in machine learning have many knobs[^knob] that can be tuned.
For example, to use the learning procedure stochastic gradient descent (SGD) we
must define an optimizer (e.g., Nesterov, Adam) and a learning rate (e.g.,
1e-5). Learning procedures specific to RL, such as DDPG, often have many more
knobs than in supervised learning, with some examples of additional knobs being
the frequency of data collection and how frequently the policy is updated.
Finding the right setting for the knobs can have a large effect on how quickly
the learning procedure solves a task, and a good configuration of knobs for one
learning procedure may be a bad configuration for another. In fact, the choice
of learning procedure itself can be considered a knob to set!


# Meta-Learning Optimizes Knobs of the Learning Procedure

While machine learning practitioners often carefully tune these knobs by hand,
if we are going to solve many tasks, it may be useful to automatic this
process. The process of setting the knobs of learning procedures via
optimization is called *meta-learning*. Algorithms that perform this optimization
problem automatically are known as *meta-learning algorithms*. Explicitly tuning
the knobs of learning procedures is an active area of research, with various
researchers looking at tuning the update rules [Learning to Learn by Gradient
Descent by Gradient Descent, RL^2, Wang], weight initialization [MAML], network
weights [hypernetworks], network architectures [WANN], and many other facets of
learning procedures.

To evaluate a setting of knobs, meta-learning algorithms consider not one task
but a distribution of tasks. For example, a distribution over supervised
learning tasks may include learning a dog detector, learning a cat detector,
and learning a bird detector. In reinforcement learning, a task distribution
could be defined as driving a car in a smooth, safe, and efficient manner,
where tasks differ by the weights they place on smoothness, safety, and
efficiency. Ideally, the task distribution is designed to mirror the
distribution over tasks that we are likely to encounter in the real world.
Since the tasks in a task distribution are typically related, information from
one task may be useful in solving other tasks more efficiently. As you might
expect, a knob setting that works best on one distribution of tasks may not be
the best for another task distribution; the optimal knob setting
**<u>depends</u>** on the task distribution.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/umrl/Meta-Learning-v2.png" width="">
<br />
<i>
An illustration of meta-learning, where tasks correspond to arranging blocks
into different types of towers. The human has a particular block tower in mind
and rewards the robot when it builds the correct tower. The robot's aim is to
build the correct tower as quickly as possible.
</i>
</p>

In many settings we want to do well on a task distribution to which we have
only limited access. For example, in a self-driving car, tasks may correspond
to finding the optimal balance of smoothness, safety, and efficiency for each
rider, but querying riders to get rewards is expensive. A researcher can
attempt to manually construct a task distribution that mimics the true task
distribution, but this can be quite challenging and time consuming. Can we
potentially get away from having to manually design such task distributions?

To answer this question, we must understand where the benefits of meta-learning
come from. The gains from meta-learning don't come for free! When we define
task distributions for meta-learning, we do so with some prior knowledge in
mind. Without this prior information, tuning the knobs of a learning procedure
is often a zero-sum game: setting the knobs to any configuration will
accelerate learning on some tasks while slowing learning on other tasks. Does
this suggest there is no way to see the benefit of meta-learning without the
manual construction of task distributions? Perhaps not! The next section
presents an alternative.

# Optimizing the Learning Procedure with Self-Proposed Tasks

If designing task distributions is the bottleneck in applying meta-learning
algorithms, why not have meta-learning algorithms propose their own tasks? At
first glance this seems like a terrible idea, because the No Free Lunch Theorem
dictates that this is impossible, *without additional knowledge*. However, many
real-world settings do provide a bit of additional information, albeit
disguised as unlabeled data. For example, in supervised learning, we might have
access to an unlabeled dataset and know that the downstream tasks will be
labeled versions of this same image dataset. In a RL setting, a robot can
interact with its environment without receiving any reward, knowing that
downstream tasks will be constructed by defining reward functions for this very
environment (i.e. the real world). Seen from this perspective, the recipe for
*unsupervised meta-learning* (doing meta-learning without manually constructed
tasks) becomes clear: given the unlabeled data, construct task distributions
from this unlabeled data or environment.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/umrl/Unsupervised-Meta-Learning-v2.png" width="">
<br />
<i>
In unsupervised meta-learning, the agent proposes its own tasks, rather than
relying on tasks proposed by a human.
</i>
</p>

Naturally the question then becomes: how do we actually construct meaningful
task distributions from this unlabeled data or environment such that we can
actually use meta-learning to set the knobs of our learning procedure in a way
that is effective for learning downstream tasks? In the case of supervised
learning, prior work on unsupervised meta-learning [Hsu 18, Khodadadeh 19]
clusters an unlabeled dataset of images and then randomly chooses subsets of
the clusters to define a distribution of classification tasks. Other work
[Jabri 2019] look at an RL setting: after exploring an environment without a
reward function to collect a set of behaviors that are feasible in this
environment, these behaviors are clustered and used to define a distribution of
reward functions. In both cases, even though the tasks constructed can be
random, the resulting task distribution is not random, because all tasks share
the underlying unlabeled data --- the image dataset for supervised learning and
the environment dynamics for reinforcement learning. *The underlying unlabeled
data are the inductive bias with which we pay for our free lunch.*

Let us take a deeper look into the reinforcement learning case. In the absence
of a known reward function or downstream tasks, what is the “best” distribution
of tasks that we might hope the agent to propose for itself for meta-learning?
And how effective might that proposed task distribution be for learning
potential downstream tasks? Is there any sense in which one unsupervised task
proposal mechanism is better than another? Understanding the answers to these
questions may guide the principled development of meta-learning algorithms with
little dependence on human supervision. Our work, UMRL [Gupta 2018], takes a
first step towards answering these questions. In particular, we examine the
*worst-case* performance of learning procedures, and derive an optimal
unsupervised meta-reinforcement learning procedure.

# Optimal Unsupervised Meta-Learning

To answer the questions posed above, our first step is to define an optimal
meta-learner for the case where the distribution of tasks is known. An optimal
meta-learner can be defined as one that achieves the minimum regret across a
known distribution of tasks --- in other words learns as quickly as possible
across this task distribution. More precisely, for a learning procedure $f$,
and the best learning procedure $f^*$, the expected regret on a task
distribution $p$ is defined as

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/umrl/labeled_regret_v2.png" width="600">
<br />
</p>

Extending this definition to the case of unsupervised meta-learning, an optimal
unsupervised meta-learner can be defined as a meta-learner that achieves the
minimum **worst-case** regret across all possible task distributions that may be
encountered in the environment. In the absence of any knowledge about the
actual downstream task, we resort to a worst case formulation. An unsupervised
meta-learning algorithm will find a single learning procedure $f$ that has the
lowest regret against an *adversarially* chosen task distribution $p$.

$$
\min_f \max_p \;\; {\rm Regret}(f,p)
$$

Our work analyzes how exactly we might obtain such an optimal unsupervised
meta-learner, and provides bounds on the regret that it might incur in the
worst case. Specifically, under some restrictions on the family of tasks that
might be encountered at test-time, the optimal distribution for an unsupervised
meta-learner to propose is **<u>uniform</u>** over all possible tasks.

The intuition for this is straightforward: if the test time task distribution
can be chosen adversarially, the algorithm must make sure it is uniformly good
over *all* possible tasks that might be encountered. As a didactic example, if
test-time reward functions were restricted to the class of goal-reaching tasks,
the regret on a particular test-time task can be tied inversely to its density
in the meta-training distribution. The less likely a task is at meta-training,
the less incentive the meta-learner has to do well on it. Now, if any one of
the goals $g_{adv}$ has lower density than the others, an adversary can propose
a task distribution solely consisting of reaching that goal $g_{adv}$ to incur
a higher regret. This implies that being uniform across all possible tasks,
prevents there from being any particular tasks that the adversary can exploit.

Now, actually proposing a uniform distribution over all possible tasks may
prove extremely challenging to optimize and sample from. In this work, we show
that several recent methods investigating mutual information based skill
discovery (e.g., DIAYN, VIC, DADS, VALOR, SMM) provide a tractable
approximation to the uniform distribution over task distributions. To
understand why this is, we can look at the form of a mutual information
objective similar to the one considered by DIAYN, between states $s$ and latent
variables $z$

$$
\mathcal{I}(s,z) = \mathcal{H}(s) - \mathcal{H}(s|z)
$$

In this objective, the first marginal entropy term is maximized when there is a
uniform distribution over all possible tasks. The second conditional entropy
term ensures consistency, by making sure that for each $z$, the resulting
distribution of $s$ is narrow. In essence, this allows us to a task
distribution that is uniform when marginalized over all tasks, but each task is
structured and consistent. This suggests constructing unsupervised
task-distributions in an environment by optimizing for mutual information based
skill discovery gives us a provably optimal task distribution, according to our
notion of min-max optimality.

While the analysis makes some limiting assumptions about the forms of tasks
encountered, we show how this analysis can be extended to provide a bound on
the performance in the most general case of reinforcement learning. It also
provides empirical gains on several simulated environments as compared to
methods which train from scratch.


<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/umrl/UMRL.png" width="">
<br />
</p>

# Summary & Discussion

In summary:

- Learning procedures are recipes for converting datasets into function
  approximators. Learning procedures have many knobs, which can be tuned by
  optimizing the learning procedures to solve a distribution of tasks.

- Manually designing these task distributions is challenging, so a recent line
  of work suggests that the learning procedure can use unlabeled data to
  propose its own tasks for optimizing its knobs.

- These unsupervised meta-learning algorithms allow for learning in regimes
  previously impractical, and further expand that capability of machine
  learning methods.

- This work closely relates to other works on [unsupervised][2] [skill
  discovery][3], [exploration][4] and [representation][5] [learning][6], but
  explicitly optimizes for transferability of the representations/skills to
  downstream tasks.

A number of open questions remain about unsupervised meta-learning:

- Unsupervised learning is closely connected to unsupervised meta-learning: the
  former uses unlabeled data to learn features, while the second uses unlabeled
  data to tune the learning procedure. Might there be some unifying treatment
  of both approaches?

- Besides UMRL, there has been relatively little theoretical analysis of
  unsupervised meta-learning. More analysis may suggest better task proposal
  mechanisms.

- Scaling unsupervised meta learning to leverage large-scale datasets and
  complex tasks holds the promise of acquiring learning procedures for solving
  real-world problems more efficiently than our current learning procedures.

## Acknowledgments

Thanks for feedback: Jake Tyo and Conor Igoe.


<hr>

[^knob]: These knobs are often known as hyperparameters, but we will stick with
    the colloquial "knob" to avoid having to draw a line between parameters and
    hyperparameters.


[1]:https://blog.ml.cmu.edu/2020/05/01/learning-to-learn-without-supervision/
[2]:https://arxiv.org/abs/1907.01657
[3]:https://arxiv.org/abs/1611.07507
[4]:https://arxiv.org/abs/1705.05363
[5]:https://arxiv.org/abs/1807.03748
[6]:https://arxiv.org/abs/1511.06434
