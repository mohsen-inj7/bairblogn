---
layout:             post
title:              "Large Scale Training at BAIR"
date:               2020-01-16 9:00:00
author:             <a href="http://people.eecs.berkeley.edu/~rliaw/">Richard Liaw</a> and <a href="https://www.linkedin.com/in/eric-liang-31308019/">Eric Liang</a>
img:                assets/tune/tune-arch-simple.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<meta name="twitter:title" content="Large Scale Training at BAIR">
<meta name="twitter:card" content="summary_image">
<meta name="twitter:image" content="https://bair.berkeley.edu/static/blog/tune/tune-arch-simple.png">

As AI research becomes more compute intensive, many AI researchers have become
squeezed for time and resources. Many researchers now rely on cloud providers
like Amazon Web Services or Google Compute Platform to access the huge amounts
of computational resources necessary for training large models.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/tune/naive-tuning.png" width="500">
<br />
<i>
</i>
</p>

In this blog post, we share our experiences in developing two critical software
libraries that many [BAIR][1] researchers use to execute large-scale AI
experiments: [Ray Tune][2] and the [Ray Cluster Launcher][3], both of which now
back many popular open-source AI libraries.


<!--more-->

# Understanding research infrastructure

To put things into perspective, let’s first take a look at a standard machine
learning workflow in industry (Figure 1). The typical *research* workflow is
actually a tight loop between steps 2 and 3, generalizing to something like
Figure 2.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/tune/tune_blog_1.jpg" width="300">
<img src="https://bair.berkeley.edu/static/blog/tune/tune_blog_2.jpg" width="300">
<br />
<i>
Figure 1 represents the typical  ML model development workflow in
<b>industry</b>.  Figure 2 represents the typical ML model development workflow
for <b>research</b>. The research workflow is typically a subsection of the
industry workflow.
</i>
</p>

This workflow is very much an iterative process and is often bottlenecked by
the experiment execution step (Figure 2, B).  Typically, an “experiment”
consists of multiple training jobs, or “trials”, where each trial is a job that
trains a single model. Each trial might train a model using a different set of
configuration parameters (hyperparameters) or a different seed.

At Berkeley, we saw that AI researchers moving to the cloud spent a lot of time
writing their own experiment execution tools that wrangled cloud-provider APIs
for starting instances, setting up dependencies, and launching experiments.

Unfortunately, despite the vast amounts of time poured into developing these
tools, these ad-hoc solutions are typically limited in functionality:

**TODO BELOW**


<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/tune/tune-arch-simple.png" width="500">
<br />
<i>
...
</i>
</p>

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/tune/training-experiment-steps.png" width="500">
<br />
<i>
...
</i>
</p>

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/tune/tune-ft-graphic.jpg" width="500">
<br />
<i>
...
</i>
</p>


[1]:https://bair.berkeley.edu/
[2]:https://ray.readthedocs.io/en/latest/tune.html
[3]:https://ray.readthedocs.io/en/latest/autoscaling.html
