---
layout:     post
title:      "Constrained Policy Optimization"
date:       2017-07-04 10:00:00
author:     Joshua Achiam
visible:    False
excerpt_separator: <!--more-->
---
(Based on joint work with David Held, Aviv Tamar, and Pieter Abbeel.)

Recently, deep reinforcement learning (RL) has enabled some remarkable achievements in hard control problems: with deep RL, agents have learned to [play video games directly from pixels](https://arxiv.org/abs/1602.01783), to control robots to [interact with objects from demonstration](https://blog.openai.com/robots-that-learn/), and even to [beat human grandmasters at Go](https://deepmind.com/research/publications/mastering-game-go-deep-neural-networks-tree-search/). Hopefully, we’ll soon be able to take deep RL out of the lab and put it into practical, everyday technologies, like UAV control and household robots. But before we can do that, we have to address the most important concern: safety.

RL agents are trained to maximize a reward signal, which must be specified in advance by a human designer. If the reward signal isn’t properly designed, the agent [can learn unintended or even harmful behavior](https://arxiv.org/abs/1606.06565). If it were easy to design reward functions, this wouldn't be an issue, but unfortunately it's fundamentally quite challenging. It's especially problematic in the context of safety, where reward functions have to play two roles: primarily, they have to describe what goal the agent should be trying to achieve; and additionally, they have to tell the agent when to abandon pursuit of the goal in order to avoid risk or harm. Mistakes in reward design could result in agents that are either too risk-averse, and therefore useless, or too risk-prone, and therefore dangerous. 

A promising approach for dealing with this issue is [constrained RL](http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf), where instead of designing agent behavior through the reward function alone, we specify both a reward function and a set of constraints for the agent. The concept for constrained RL has been around for a long time, but our work on constrained policy optimization (CPO) is the first algorithm that makes it practical to combine constrained RL with deep learning.

We’ll split this blog post up into three sections: in the first part, we’ll explain why it’s hard to address safety concerns through reward design, and why constraints make things easier. Also, in the course of that discussion, we’ll highlight some key properties that we want a “safe” RL algorithm to have. Then in the second part, we'll introduce some background on policy search methods, and describe our theoretical development which leads to CPO. Finally, we'll describe CPO and show how our theoretical results ensure that CPO satisfies the properties we want from a "safe" RL algorithm.

<!--more-->

# Designing Behavior Through the Reward Function

Suppose we want to train an RL agent to do a task, and we want it to be “safe” somehow. In order to train this agent, we have to first design a reward function which is maximized by the desired behavior. This is a non-trivial challenge, owing to the need to choose tradeoffs: how much should the agent be rewarded for success or penalized for undesired (or “unsafe”) behavior? To understand why this is challenging, we have to think of two things: the agent’s policy at optimum, and the dynamics of the learning process. 

Let’s consider an example based on a real-world use case, although we’ll simplify it for the sake of discussion: an autonomous car, which is rewarded for getting to its destination and penalized for collisions. From a practical perspective, we will likely require that the collision frequency for autonomous cars be less than some pre-selected threshold, for the sake of safety. If the frequency of collisions is below that threshold, we’ll consider the car “safe,” and otherwise, it’s “unsafe.”

Suppose that the car gets a reward of +1 for getting to its destination, and a reward of -X for being involved in a collision. After being fully trained, the policy will still get into collisions sometimes, but the frequency of collisions will depend on X. If X is larger, the frequency of collisions will be smaller, and vice versa. But this intuition is not *rigorous*, and we have no formula for predicting collision frequency from X; as a result, we cannot pick X in advance to attain a design specification on safety. In order to learn a safe policy with current deep RL methods, we would have to train many different policies with different values of X, until we find one that works. 

This is bad because of the learning dynamics. RL agents learn by trial and error, and they go through a sequence of intermediate exploration policies before ultimately converging to optimum. Although sometimes high-fidelity simulators of the real world can be used to accelerating training, RL agents for the real world will ultimately have to do some of their exploration in the real world. This means that during the learning process to find our safe autonomous car policy, some choices of X will mean putting unsafe policies in real driving situations—which would clearly be unacceptable! Importantly, even choices of X that lead to safe policies at optimum could result in unsafe exploration somewhere between the beginning and end of training. 

In many cases in the real world, like in the example, it’s easier to specify constraints than choose tradeoffs: a system is often considered safe if it behaves predictably, in the sense that failure modes are rare and happen less often than some known chosen at design time. This motivates constrained RL as the natural way to incorporate safety into RL. Furthermore, from thinking about the learning dynamics, we want to make sure that *every exploration policy* is also constraint-satisfying. 

# Local Policy Search

A standard way to learn policies is by local policy search, where we iteratively improve policies until convergence to optimum. In the policy improvement step, we enforce a locality constraint: the new policy must be close to the old one in some sense. We can express this inner-loop optimization problem as

$$\pi_{k+1} = \arg \max_{\pi \in \Pi_{\theta}} J(\pi) \; : \; D(\pi, \pi_k) \leq \delta$$

where 

* $$k$$ denotes iteration,
* $$\pi$$ is a policy,
* $$\Pi_{\theta}$$ is the set of policies parametrized by variables $$\theta$$ (for instance, neural networks of a fixed architecture), 
* $$J(\pi)$$ is the performance measure (average cumulative discounted reward over trajectories), 
* $$D$$ is some measure of distance between policies, 
* and $$\delta$$ is a step size. 

So, why do we take this *local* policy search approach? Why not try to solve the RL problem directly, and pick $$\pi$$ to maximize $$J(\pi)$$? The problem is that *estimating $$J(\pi)$$ is hard.* The best way to estimate $$J(\pi)$$ is to run policy $$\pi$$ in the environment, collect and total up the rewards the agent receives, and then average over trajectories. Doing this for every possible candidate policy, and then selecting the best policy, would be unnecessarily expensive.

Wonderfully, local approximations for $$J(\pi)$$ are easy. So if we collect data for a single policy and then approximate $$J(\pi)$$ in a neighborhood around that one, we can optimize the approximate objective over that neighborhood, and probably produce a good policy update. It's standard to approximate $$J$$ around the current policy iterate $$\pi_k$$, which means that we use each iterate $$\pi_k$$ as an exploration policy.  

To illustrate, policy gradient methods are local policy search algorithms that use first-order approximations: 

$$J(\pi) \approx J(\pi_{k}) + g^T_k (\theta - \theta_k)$$

where $$\pi$$ has parameters $$\theta$$, $$\pi_{k}$$ has parameters $$\theta_{k}$$, and $$g_{k} = \nabla_{\theta} J(\pi_{k})$$ is the policy gradient. 

Our main theoretical result is about another kind of approximation, which turns out to be quite useful in practice. The mathematical details are a little complex, so we'll summarize instead: there's a function, $$\mathcal{L}(\pi, \pi')$$, which is known to relate to the difference in performance between two policies $$\pi$$ and $$\pi'$$. This function--which we'll call the *surrogate advantage function*--is easy to estimate using data collected on policy $$\pi$$, allowing us to optimize over $$\pi'$$ without collecting data on $$\pi'$$. Our result is that we are able to establish new bounds on the quality of that approximation:

$$| J(\pi') - \left( J(\pi) + \mathcal{L}(\pi, \pi') \right) | \leq \alpha \sqrt{\bar{D}_{KL}(\pi', \pi)}$$

for a particular coefficient $$\alpha$$. Here, $$\bar{D}_{KL} (\pi', \pi)$$ is the average [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between policies (where the average is over trajectories from $$\pi$$). This tightens [previously discovered bounds](https://arxiv.org/abs/1502.05477), which had a worst-case KL-divergence instead of an average KL-divergence. 

The gain is that we can now characterize the performance of **[trust region methods](https://arxiv.org/abs/1502.05477),** a family of local policy search algorithms that use updates of the form


$$\pi_{k+1} = \arg \max_{\pi \in \Pi_{\theta}} \mathcal{L}(\pi_k, \pi) \; : \; \bar{D}_{KL} (\pi, \pi_k) \leq \delta.$$

Trust region methods are known to perform extremely well in practice for training neural network policies, and are able to avoid the issue of performance collapse that policy gradient algorithms can sometimes suffer from (see [Duan et al., 2016](https://arxiv.org/abs/1604.06778)).  Our bound gives us some intuition as to why: the performance of $$\pi_{k+1}$$ is guaranteed to satisfy

$$J(\pi_{k+1}) \geq J(\pi_k) - \alpha \delta,$$

which means that for $$\delta$$ small enough, trust region algorithms are approximately monotonic in policy performance.

# Constrained Policy Updates via CPO

Now, at last, we can discuss CPO: a method for local policy search that incorporates constraints. Recall that we wanted a "safe" RL algorithm to have two essential properties: 

1. It should incorporate constraints.
2. Those constraints should be enforced not just for the policy at optimum, but for every exploration policy as well.

To address the first property, we consider a set of constraints of the form:

$$J_{C_i} (\pi) \leq d_i, \;\;\;\;\; \text{for } i \in \{1,...,K\}$$

where $$C_i$$ is some *auxilliary cost function*, $$J_{C_i} (\pi)$$ is the average cumulative discounted auxilliary cost accrued while acting according to $$\pi$$, and $$d_i > 0$$ is a given limit. This is a fairly flexible definition for constraints and encompasses many natural use cases. 

For the second property, ideally we would incorporate the constraints directly into our local policy search update:


$$\begin{align} \pi_{k+1} = \arg \max_{\pi \in \Pi_{\theta}} \;\; & J(\pi) \\ \text{subject to } & D(\pi, \pi_k) \leq \delta \\ & J_{C_i} (\pi) \leq d_i \;\;\;\;\; \text{for } i \in \{1, ..., K\}. \end{align}$$

But this suffers from the policy performance evaluation problem that we discussed previously.  Instead, we'll solve a particular approximation, and use the average KL-divergence for $$D$$:

$$\begin{align} \pi_{k+1} = \arg \max_{\pi \in \Pi_{\theta}} \;\; & \mathcal{L}(\pi_k, \pi) \\ \text{subject to } & J_{C_i} (\pi_k) + \mathcal{L}_{C_i} (\pi_k, \pi) \leq d_i, \;\;\;\; \text{for } i \in \{1, ..., K\}, \\
& \bar{D}_{KL}(\pi, \pi_k) (\pi) \leq \delta. \end{align}$$

where $$\mathcal{L}_{C_i}$$ is the surrogate function for constraint $$i$$. **This is the CPO update.** Because it is a trust region method, it inherits the performance near-improvement guarantee we discussed in the previous section. Furthermore, because of our choice of approximation for the constraint, it comes with guarantees on near-constraint satisfaction:

$$J_{C_i} (\pi_{k+1}) \leq d_i + \alpha_{C_i} \delta,$$

where $$\alpha_{C_i}$$ is the bounding constant for constraint $$i$$. 

[In our paper](https://arxiv.org/abs/1705.10528), we describe a way to efficiently numerically solve the CPO update equation, and show that CPO can train neural network policies to solve several constrained robot locomotion tasks. Our experimental results verify our theoretical results: CPO is empirically nearly constraint-satisfying, and generally achieves monotonic improvement in policy performance. 

In the interest of encouraging follow-up work, [we've also open-sourced our code!](https://github.com/jachiam/cpo)

# Conclusions

In this work, we described a method for doing reinforcement learning with constraints which is theoretically well-founded and empirically successful. Our method, CPO, represents a step towards bringing RL out of the lab and into the real world, by enabling us to explicitly specify safety requirements when training RL agents. 


