---
layout:             post
title:              "Learning the Language of Proteins"
date:               2019-11-04 9:00:00
author:             <a href="https://people.eecs.berkeley.edu/~nthomas/">Neil Thomas</a> and
                    <a href="https://nickbhat.github.io/">Nicholas Bhattacharya</a> and
                    <a href="https://rmrao.github.io/">Roshan Rao</a><br>
img:                assets/tape/proteins_xkcd.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<!--
https://bair.berkeley.edu/static/blog/tape/
<p style="text-align:center;">
<img height="250" src="https://bair.berkeley.edu/static/blog/coordination/2 Alice _ You successful coord.gif">
<img height="250" src="https://bair.berkeley.edu/static/blog/coordination/3 â€“ Bob Charlie Long Way.gif">
<br>
<i>
Left: Alice (green) and you (blue) passing onions. Right: Bob (green) and
Charlie (blue) taking the long way.
</i>
</p>
-->

  <p> In Natural Language Processing (NLP), BERT and then GPT-2 showed that large models trained on enormous amounts of
  unlabeled data are able to learn powerful representations of language. These representations have been shown to encode information about syntax and semantics, and can <a href="https://openai.com/blog/better-language-models/">generate realistic-sounding text</a>. In this blog post we ask the question: Can
  similar methods be applied to biological sequences, specifically to proteins? If so, to what degree do they help on downstream prediction problems that are relevant to biologists? </p>

  <p> We show results from our recent work on TAPE: Tasks Assessing Protein Embeddings <a href="https://www.biorxiv.org/content/10.1101/676825v1">(preprint)</a> <a href="https://github.com/songlab-cal/tape
">(github)</a> which
  benchmarks protein representations learned by various neural architectures and self-supervised losses and also discuss the interesting challenges protein modeling presents
  to the ML community. </p>

<!--more-->

<h1 id="motivation"> Proteins Do Everything </h1>

<!-- https://xkcd.com/1430/ -->
<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/tape/proteins_xkcd.png" height="200">
<br />
</p>

  <p> Well, pretty much. As you read this, hemoglobin in your blood is moving oxygen to your muscles, transporter proteins are pumping salt and potassium into and out of your cells, and photoreceptor proteins in your eye are distinguishing between the boundaries of the letters in this sentence. Understanding what causes the disruption of a protein's natural function can help us understand the molecular basis of disease, and lead to better treatments. Across many organisms, evolution has found proteins that serve as solutions to common problems, such as antimicrobial resistance. The trick is finding these proteins, which are often hiding in the genomes of bacteria that live in extreme environments. In addition to networks of naturally-evolved proteins performing the necessary functions to keep you alive, artificially developed proteins can be spliced into bacterial genomes and put to work diligently producing insulin, breaking down plastic waste, and creating laundry detergent. </p>

 <h1 id="background"> Proteins 101: In Case You Ditched Alot in High School </h1>

  <p>A protein is a linear chain of amino acids connected by covalent bonds. There are <a href="https://en.wikipedia.org/wiki/File:Amino_Acids.svg">20 standard amino acids</a>. This "alphabet" lets us represent a protein as a sequence of discrete tokens, just as we might encode a sentence of English. This discrete sequential representation is known as a protein's <i>primary</i> structure. </p>

  <!-- https://www.wikiwand.com/en/Amino_acid -->
<!--   <p style="text-align:center;">
  <img src="amino_acids.svg" width=400>
  <br />
  <i> Figure 1: Our alphabet: the 20 standard amino acids (+ "U"/Selenocysteine), grouped according to their biophysical properties. </i>
  </p> -->

  <p style="text-align:center;">
  <img src="https://bair.berkeley.edu/static/blog/tape/simple_polypeptide.svg" width="400">
  <br />
  <i> A protein fragment represented as a sequence of letters in the amino acid alphabet. </i>
  </p>

  <p> In a cell, however, a protein is a three-dimensional molecular object. Since a protein's function is
  heavily dependent on its structure, understanding this 3D structure is key to piecing together its biological role. The local
  topology of a protein is known as <i>secondary</i> structure, and plays a role in the behavior of different segments
  of the protein. The global topology of a protein is known as <i>tertiary</i> structure, and determines the behavior
  of the protein as a whole. </p>

  <p style="text-align:center;">
  <img src="https://bair.berkeley.edu/static/blog/tape/protein_structure.png" height="920">
  <br>
  <i>
  Protein structure at varying scales: primary structure (sequence), secondary
  structure (local topology), and tertiary structure (global topology).
  </i>
  </p>

<h1 id="walkthrough"> ... but what does this protein do? </h1>

  Let's say we hand you this sequence.

  <!-- https://www.uniprot.org/uniprot/P42212 -->
  <p style="text-align:center;">
  <img src="https://bair.berkeley.edu/static/blog/tape/gfp_sequence.png" height="200">
  <br />
  </p>

  Your goal is to specify the 3D coordinates for each position in the sequence,
  corresponding to a biophysically reasonable conformation. In this case, the
  structure looks like this.

  <!-- https://upload.wikimedia.org/wikipedia/commons/e/e4/GFP_structure.png -->
  <p style="text-align:center;">
  <img src="https://bair.berkeley.edu/static/blog/tape/GFP_structure.png" height="400">
  <br>
  <i> The 3D structure of the <a href="https://en.wikipedia.org/wiki/Green_fluorescent_protein">green fluorescent protein (GFP)</a>, isolated from the jellyfish Aequorea victoria. The gradient in color represents the index within the sequence from start to end.
  </i>
  </p>

  <p>
  This is a fundamental challenge of biochemistry, arguably one of the hardest prediction
  tasks out there. So hard, in fact, that <a href="https://moalquraishi.wordpress.com/2018/12/09/alphafold-casp13-what-just-happened/">DeepMind wants to get in on the action</a>. Which is, let's face it, probably why anyone is reading this.
</p>
  <p>
  To do supervised learning for this problem, you need labels. In this case, a label is the three-dimensional coordinates for every atom in the molecule. Labelling a single protein is the result of a labor-intensive, resource-intensive and time-consuming process that can only be performed by experts (yes, to the O(1) structural biologists reading, we're talking about you). Every year, we solve more and more structures, but most of these are focused on already well-characterized proteins, while the number of known <i>sequences</i> is growing exponentially compared to the number of known structures.
  </p>
  <p>
  Clearly we would like to bring in some outside information.
</p>

<h1 id="unlabeled"> How I Learned To Stop Worrying and Love Unlabelled Data </h1>

<p>
  The good news is that there are <i>lots</i> of unlabelled protein sequences available - from tens to hundreds of millions, depending on the database you use. If we can extract useful information from these large databases, this could provide us with a powerful source of signal.
</p>
<p>
  Currently the dominant paradigm for using these databases is <i>sequence alignment</i>. We take our query sequence and scan it against the entire database, looking for other "cousin" sequences that are evolutionarily related (i.e. are descendants of a common ancestor). In the simplest case, if we find a cousin sequence with a solved structure, we can map the cousin's structure onto the aligned positions of our query sequence.
</p>

  <!-- http://pfam.xfam.org/family/GFP#tabview=tab3 -->

  <p style="text-align:center;">
  <img src="https://bair.berkeley.edu/static/blog/tape/gfp_alignment.png" height="400">
  <br />
  <i> Partial sequence alignment for the <a href="http://pfam.xfam.org/family/GFP">GFP protein family</a>. The column on the left has the ID of the sequence, followed by the positions of that sequence represented in the alignment. Colors correspond to amino acid groupings based on biophysical properties:
    purple for hydrophobic
    <font color="9a93ff"> (C, A, V, L, I, M, F, W)</font>,
    red for charged
    <font color="f9999b"> (D, E, R, K)</font>,
    green for polar + uncharged
    <font color="a2ff96"> (S, T, N, Q)</font> etc. Columns with a consistent color represent evolutionarily conserved positions.
    Dots (.) and dashes (-) correspond to gaps, i.e. where an insertion or deletion has been observed.

  </i>
  </p>

<p>
  What do evolutionary relationships buy us? Evolution is constrained; if a modification to a protein causes its structure to be disrupted, the organism will lose that function and be impaired. Thus we hope that the extracted set of cousins from the database show us where evolution had free reign, where it had some room to wiggle, and where its hands are completely tied. These representations of <i>protein families</i>, aptly called <i>Multiple Sequence Alignments</i>, are crucial inputs to protein structure prediction pipelines. Intuitively, positions that are close in 3D space will co-evolve (i.e. a mutation at one position will commonly co-occur with an accommodating mutation at the other).
</p>

<p>
  There are many variations on this theme, but all boil down to using an evolutionary model to either query for related sequences or extract features from the model directly. As always, evolution provides a powerful paradigm for thinking about biological problems!

</p>

<h1 id="self-supervision">Learning the Language of Proteins via Self-Supervision</h1>

<p>
Large corpuses... difficulty of getting labels... sequential tokens... sound familiar? All of these properties of biological sequences have a nice analogy to NLP. One of the big recent breakthroughs in NLP is the use of <i>self-supervised</i> pretraining, a way of bootstrapping labels from unlabelled data.
</p>

<p>
Simply put, we sample a sequence from our dataset

<blockquote>
  Let's stick to improvisation in this skit
</blockquote>

We mask out a random portion. This masked sequence is the input to our neural network model.

<blockquote>
  Let's stick to [?] in this skit
</blockquote>

We have our model output a probability distribution over our vocabulary, corresponding to the probability that the word was masked, and use a crossentropy loss to penalize our model for predicting the wrong word. By observing many, many, such sequences (say, all of Wikipedia), models such as BERT can learn features from sequences that transfer well to a variety of downstream tasks that involved knowledge of syntax and semantics.
</p>

<p style="text-align:center;">
  <img src="https://jalammar.github.io/images/BERT-language-modeling-masked-lm.png" height="400">
  <br/>

    <i>
      Masked language modeling with the transformer (<a href="https://jalammar.github.io/illustrated-bert/">source</a>)
  </i>
</p>

<p>
  For proteins, we can do the something similar at the level of amino-acid. Sample a known protein like our friend GFP:

<blockquote>
  MSKGEELFTGVVPILVELDGDVNGHKFSVS...
</blockquote>
Mask out a random portion, and ask the model to fill in the rest.
<blockquote>
  MSKGE?LFT?VVP?ILVELDGDV?GHKFSVS...
</blockquote>

</p>

<!-- https://moalquraishi.wordpress.com/2018/02/15/protein-linguistics/ -->
Why should this syntax be learnable? To paraphrase an idea from <a href="https://moalquraishi.wordpress.com/2018/02/15/protein-linguistics/">Mohammed AlQuraishi</a>, the ability to predict protein properties by examining raw sequences is predicated on the <b>linguistic hypothesis</b>:
<blockquote><i>The space of naturally occurring proteins occupies a learnable manifold. This manifold emerges from evolutionary pressures that heavily encourage the reuse of components at many scales: from short motifs of secondary structure, to entire globular domains. </i></blockquote>

Which, since we're talking about copying genetic material, can be stated even more simply as the <b><a href="https://www.youtube.com/watch?v=gXlfXirQF3A">Lehrer-Lobachevsky</a> principle</b>:
<blockquote><i>Plagiarize, plagiarize, plagiarize!</i></blockquote>

<p>
  By observing many sequences, our model can gain an implicit understanding of how proteins are constructed: the portions of sequence that are reused, interactions between pairs of positions, evolutionarily conserved regions. In principle this could yield tremendous gains. Self-supervision gives us a way to transfer information between distant groups of proteins. Where alignments suffer to characterize families of proteins that are underrepresented in the database, self-supervised models could use partial information learned from *other* families of proteins to provide informative features.
</p>

<p>
  Multiple groups had this idea somewhat concurrently, resulting in a <a href="https://www.biorxiv.org/content/10.1101/589333v1">flurry</a> <a href="https://arxiv.org/abs/1902.08661">of</a> <a href="https://www.biorxiv.org/content/10.1101/622803v2">work</a> applying language modeling to proteins. Each one experiments with different pretraining corpuses, uses varying scales of computational resources, and evaluates on different downstream tasks. To assess the state of self-supervision for protein transfer learning, we fixed those variables and developed TAPE: Tasks Assessing Protein Embeddings, inspired by the "sticky" NLP benchmark <a href="https://gluebenchmark.com">GLUE</a>. In TAPE, we benchmark a variety of self-supervised models drawn from both the protein and NLP literature on a diverse array of difficult downstream tasks.
</p>

 <h1 id="self-supervision-help"> Does self-supervision help? </h1>

 We'll take as our case-study the task of contact prediction.
  <p style="text-align:center;">
  <img src="https://bair.berkeley.edu/static/blog/tape/contact.png" height="200">
  <br />
  <i>
      Visualization of the contact map (right) for a barrel protein structure (left). Amino acids within 8<span>&#8491;</span> are considered to be in contact
  </i>
  </p>
  <p>Contact prediction is a task where each pair of amino acids is mapped to a binary 0/1 label. A 1 indicates that the pair of amino acids are nearby in 3D space (<span>&lt;</span>8<span>&#8491;</span>). Accurate contact maps provide powerful global information; e.g. they facilitate robust modeling of the full 3D structure.</p>


  <h1 id="pretraining-helps"> Yes, it helps </h1>

  <p style="text-align:center;">
    <img src="https://bair.berkeley.edu/static/blog/tape/original_map.png" width="200" />
    <img src="https://bair.berkeley.edu/static/blog/tape/nopretrain_map.png" width="200" />
    <img src="https://bair.berkeley.edu/static/blog/tape/pretrain_map.png" width="200" />
    <br />
    <i>  True and predicted contacts for chain 1A of a <a href="https://www.rcsb.org/structure/3GKN">Bacterioferritin</a>. The true contact map (left), predicted contacts from an unpretrained LSTM (center), predicted contacts from a pretrained LSTM (right). Blue pixels represent true positives, red pixels represent false positives, and the darkness of the color represents the confidence of the model in its prediction.
    </i>
  </p>

Qualitatively, contact prediction becomes much sharper with pretraining (center to right). This theme is consistent across all our benchmark tasks, and across all models: <b>pretraining on a large corpus of protein sequences substantially improves results compared to unpretrained models</b> that only have access to the supervised downstream task.

  <h1 id="non-neural"> , but it leaves a lot on the table </h1>

  <p style="text-align:center;">
    <img src="https://bair.berkeley.edu/static/blog/tape/original_map.png" width="200" />
    <img src="https://bair.berkeley.edu/static/blog/tape/alignment_map.png" width="200" />
    <img src="https://bair.berkeley.edu/static/blog/tape/pretrain_map.png" width="200" />
    <br />
    <i> The true contact map for the same protein as above (left), predicted contacts from a model with alignment-based features (center), predictions from a <b>pretrained</b> LSTM (right).
    </i>
  </p>

  An intriguing result from the benchmark was that <b>features learned via self-supervised pretraining were substantially outperformed by (non-neural) alignment-based features</b> on structural tasks. We can see qualitatively that the predicted contact map from the alignment-based features are sharper, and predicts contacts that the pretrained LSTM misses completely. Why alignment-based features outperform self-supervised features is an open question, but it is an indication that there is a massive opportunity for improving pretraining tasks and architectures to fully take advantage of protein sequence data.

 <h1 id="future">"<a href="https://moalquraishi.wordpress.com/2019/04/01/the-future-of-protein-science-will-not-be-supervised/">The Future Will Not Be Supervised</a>"</h1>

 We think this is a very exciting time for machine learning on proteins. Not only does better protein modeling have huge potential for real-world impact, it also presents interesting machine learning challenges. Proteins are long, variable length sequences which fold into compact structures, leading to interactions between amino acids that may be dozens or even hundreds of positions apart. Much of our knowledge is centered around well-characterized proteins, so filling in the holes between sequences that haven't been sampled very densely (known as the "<a href="https://www.pnas.org/content/112/52/15898">dark proteome</a>") is a unique challenge for model generalization. As a testing ground for self-supervision, proteins are particularly fruitful, as the rate of collection of sequence data far outstrips the rate of experimental characterization.

 For all these reasons, we think the time is ripe for machine learning practitioners to experiment with protein data.

  <h1 id="getting-involved"> Getting involved in protein modeling </h1>

  If you'd like to get started, take a look at <a href="https://github.com/songlab-cal/tape">our GitHub</a> which has code (tensorflow for now, pytorch support coming soon!), as well as pretrained models and training data (TFRecords and JSON formats available).

  In the future, we're looking to expand the TAPE benchmark, continuing to develop architectures and pretraining tasks that learn more from protein sequences, and working with biologists to leverage these models for experimental data.

  <h1 id="acknowledgements"> Acknowledgements </h1>

  This work was done in collaboration with Rocky Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. This work also benefitted significantly with input from a number of people with different backgrounds, so we'd also like to thank Philippe Laban, David Chan, Jeffrey Spence, Jacob West-Roberts, Alex Crits-Cristoph, Surojit Biswas, Ethan Alley, Mohammed AlQuraishi, and Kevin Yang.
