---
layout:             post
title:              "Learning the Language of Proteins"
date:               2019-11-04 9:00:00
author:             <a href="">Neil Thomas</a> and
                    <a href="">Nicholas Bhattacharya</a> and
                    <a href="https://rmrao.github.io/">Roshan Rao</a><br>
img:                assets/tape/proteins_xkcd.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<!--
<p style="text-align:center;">
<img height="250" src="https://bair.berkeley.edu/static/blog/coordination/2 Alice _ You successful coord.gif">
<img height="250" src="https://bair.berkeley.edu/static/blog/coordination/3 â€“ Bob Charlie Long Way.gif">
<br>
<i>
Left: Alice (green) and you (blue) passing onions. Right: Bob (green) and
Charlie (blue) taking the long way.
</i>
</p>
-->

  <p> In Natural Language Processing (NLP), BERT and then GPT-2 showed that large models trained on enormous amounts of
  unlabeled data are able to learn powerful representations of language. These representations have been shown to encode information about syntax and semantics, and can <a href="https://openai.com/blog/better-language-models/">generate realistic-sounding text</a>. In this blog post we ask the question: Can
  similar methods be applied to biological sequences, specifically to proteins? If so, to what degree do they help on downstream prediction problems that are relevant to biologists? </p>

  <p> We show results from our recent work on TAPE: Tasks Assessing Protein Embeddings <a href="https://www.biorxiv.org/content/10.1101/676825v1">(preprint)</a> <a href="https://github.com/songlab-cal/tape
">(github)</a> which
  benchmarks protein representations learned by various neural architectures and self-supervised losses and also discuss the interesting challenges protein modeling presents
  to the ML community. </p>

<h1 id="motivation"> Proteins Do Everything </h1>

<!-- https://xkcd.com/1430/ -->
<p style="text-align:center;">
<img src="proteins_xkcd.png" height=200>
<br />
</p>



<!--more-->
