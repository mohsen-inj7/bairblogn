---
layout:     post
title:      Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics.
date:       2017-01-06 9:00:00
author:     Jeff Mahler
excerpt_separator: <!--more-->
---

[![IMAGE ALT TEXT](http://i.imgur.com/BwJ88wo.jpg)](https://www.youtube.com/watch?v=i6K3GI2_EgU "Dex-Net 2.0: 99% Precision Grasping")

This blog summarizes new results that will be presented at the Robotics: Science and Systems Conference at MIT in July 2017 [[1]](https://arxiv.org/abs/1703.09312).

Reliable robotic grasping across many objects is challenging due to sensor noise, which makes it difficult for a robot to precisely infer physicals properties of such as object shape, pose, material properties, mass, and the locations of contact points between the fingers and object. Recent results suggest that deep neural networks trained on large datasets of human grasp labels [[2]](https://www.researchgate.net/profile/Jeannette_Bohg/publication/282856821_Leveraging_Big_Data_for_Grasp_Planning/links/561f6fde08aecade1ace2dc5.pdf) or trials of grasping on a physical system [[3]](https://arxiv.org/abs/1603.02199) can be used to plan successful grasps across a wide variety of objects directly from images [[4]](https://arxiv.org/abs/1608.02239) with no explicit modeling of physics, similar to generalization results seen in computer vision. However, current methods require months of execution time on a physical system or tedious human hand-labeling of millions of examples to learn to grasp with up to 90% success.

An alternative is to use Cloud Computing to rapidly compute grasps across a large dataset of object mesh models [[5]](http://www.coreygoldfeder.com/papers/ICRA09.pdf) using physics-based models of grasping [[6]](https://www.researchgate.net/profile/JC_Jeff_Trinkle/publication/260403016_Grasping_28_Grasping/links/55451e690cf23ff71686998e.pdf). These methods rank potential grasps according to success metrics, such as whether or not the grasp can resist arbitrary forces and torques, evaluated under perturbations in properties such as object pose and friction coefficient to model the effects of sensor noise [[7]](http://www.cs.columbia.edu/~allen/PAPERS/weisz_icra12.pdf). However, they make the strong assumption of a perception system that estimates these properties either perfectly or according to known Gaussian distributions. In practice, these perception systems are slow, prone to errors, and may not generalize well to new objects. Thus, despite over 30 years of research, in practice it is common to plan grasps using heuristics such as cylinder detection in applications such as home decluttering [[8]](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi864WCua3UAhUS12MKHYBxBEsQFggrMAA&url=http%3A%2F%2Fwww.willowgarage.com%2Fsites%2Fdefault%2Ffiles%2Fgrasp_pipeline.pdf&usg=AFQjCNEOePSTZA8hxyt2ME-VyUuLYWevAA&sig2=ddAkMNcZ1pYW1micCmXfZA) and the Amazon Picking Challenge [[9]](https://arxiv.org/abs/1610.05514).

<!--more-->

# Dex-Net 2.0: A Hybrid Approach
We propose an alternative approach that attempts to leverage the best of both methods: to use deep Convolutional Neural Networks (CNNs) to generalize grasps across a wide variety of objects and physics-based models to quickly generate the massive datasets of images and grasps needed to reliably train them.

Rather than attempt to precisely infer quantities such as 3D object shape and pose from images, we propose to construct a probabilistic model that can generate synthetic point clouds, grasps, and grasp success metrics from datasets of 3D object meshes [[10]](http://goldberg.berkeley.edu/pubs/icra16-submitted-Dex-Net.pdf) using physical models of grasping, image rendering, and camera noise. Our key insight is that possible correlations between grasp success and geometric features in point clouds, such as handles and cylinders, will be evident in samples from the model, and Deep CNNs may be able to leverage these correlations using a hierarchical set of convolutional filters for point clouds similar to the hierarchies of Gabor-like filters learned for image classification [[11]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).

We formalize and study this approach in our paper, ["Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics."](https://arxiv.org/abs/1703.09312) In the paper we introduce the Dexterity Network (Dex-Net) 2.0, a dataset of 6.7 million robust grasps and point clouds with synthetic noise generated from our probabilistic model of grasping rigid objects on a tabletop with a parallel-jaw gripper. We develop a deep Grasp Quality Convolutional Neural Network (GQ-CNN) model and train it on Dex-Net 2.0 to estimate the probability of success (or *robustness*) of grasps directly from point clouds. Then we use the GQ-CNN to plan grasps on a physical robot by sampling a set of grasp candidates from an input point cloud with edge detection and executing the most robust grasp estimated by the GQ-CNN:

![alt text](http://i.imgur.com/ChTDZKW.png "Dex-Net 2.0 Grasping Policy")

# Grasp Quality Convolutional Neural Networks
The GQ-CNN estimates the probability of success for a grasp specified as a 3D location and angle from a point cloud. The depth image is rotated and translated to align the grasp center and angle to remove the need to learn rotational invariances. When trained on Dex-Net 2.0, the GQ-CNN learns a set of first-layer convolutional filters that appear to detect image gradients at various scales. Filters can be organized into two classes: coarse oriented gradient filters that may be useful for estimating collisions between the gripper and object and fine vertical filters that may be useful for estimating surface normals at the locations of contact between the fingers and object:

![alt text](http://i.imgur.com/OelLCuA.png "Grasp Quality Convolutional Neural Network")

# Experiments with the ABB YuMi
To evaluate the Dex-Net 2.0 grasp planner on a physical robot, we ran over 1,000 trials of grasping on an ABB YuMi to investigate:
1. **Model Performance:** Can a GQ-CNN trained entirely on synthetic data for a set of known objects be used to successfully grasp the objects on a physical robot with no additional training?
2. **Generalization:** Can the GQ-CNN be used to successfully grasp novel objects that were not seen in training?

### Model Performance
We first measured the ability of our method to plan grasps that could maintain a grasp on the object while lifting the object, transporting it, and shaking it within the gripper. We used a set of a eight 3D printed objects with known shape, center of mass, and frictional properties to highlight differences between our physical models and grasping on the physical robot. In particular we chose objects with adversarial geometry for two-finger grippers such as smooth, curved surfaces and narrow openings to explore failure modes.

We found that the Dex-Net 2.0 grasp planner could achieved up to 93% success on the physical robot and was 3x faster than a method that matched the exact object shape to the point cloud. The results suggest that our physics-based model is a useful approximation of reality for grasp planning when object properties are known and that the GQ-CNN can be used to plan highly precise grasps. Here's an example: 

![alt text](https://tctechcrunch2011.files.wordpress.com/2017/05/dex-net.gif "Precision Grasp Planned by Dex-Net 2.0")

### Generalization
We also evaluated the ability to generalize to previsouly unseen objects by testing on a set of ten novel household objects never seen in training. On this dataset, the Dex-Net 2.0 planner had the highest success rate of all methods. However, after analyzing the data further we found a surprising result: the GQ-CNN predicted zero false positives out of 29 grasps predicted to succeed. In other words, when the GQ-CNN was highly confident that a grasp would work, it was always correct. To further investigate this phenomenon, we ran a follow up experiment on 40 novel objects including objects with moving parts, such as a can opener, and deformability, such as a washcloth. The Dex-Net 2.0 planner had 99% precision on these objects, predicting only one false positive out of 69 grasps predicted to succeed. 

# Discussion and Limitations
The results of grasp planning with Dex-Net 2.0 suggest that it is possible to achieve highly reliable grasping across a wide variety of objects by training neural networks with only synthetic data generated using physical models of grasping and image formation. We hope this will facilitate its use in applications such as warehousing and manufacturing, where the robot can take a secondary action such as ask a human for help or poke objects when it is not confident that a grasp will succeed.

However, there are several limitations of the current method:
1. **Sensor Capabilities.** There are some sources of noise on the physical depth camera, such as missing data, that are not accounted for by the Dex-Net 2.0 model. Furthermore, depth cameras cannot see objects that are transparent or flat on a table.
2. **Model Limitations.** The physical model of grasping used by Dex-Net 2.0 considers fingertip grasps of rigid objects. Grasping strategies such as pinching a flat piece of paper into the gripper or hooking an object with a finger are not accounted for.
2. **Single Objects.** The method is designed to only grasp objects in isolation. We are currently working on extending the Dex-Net 2.0 model to grasping objects from a pile.
3. **Task-Independence.** The method plans grasps that can be used to robustly lift and transport an object but does not consider use cases of an object such as
exact placement, stacking, or connecting it to another object in assembly which require more precise grasps. We are research possible extensions with task-based grasp quality metrics, dynamic simulation, and learning from demonstration.

# Dataset and Code Release
To make progress toward overcoming these limitations, we plan to release a subset of our code, datasets, and the trained GQ-CNN weights over summer 2017 which we hope will facilitate further research and comparisons.

We are aiming for the following releases and dates:
* **GQ-CNN Package:** Dex-Net 2.0 GQ-CNN training dataset with 6.7 million datapoints and ROS integration. *June 19, 2017.*
* **Read Only Dex-Net 2.0:** Dex-Net 2.0 Database with Python API that can be used to generate GQ-CNN training datasets for new cameras and grippers. *July 11, 2017.*
* **Full Dex-Net 2.0:** Dex-Net 2.0 Python API to create new databases with custom 3D models and custom grasp success metricss and a Web API to upload new models: *September 1, 2017.*

We also plan to keep a leaderboard of performance on the Dex-Net 2.0 dataset to investigate improvements to the GQ-CNN architecture, since our best models achieve only 91% classification accuracy. We will also evaluate the performance of models that signficantly outperform previous methods on the physical robot. We invite researchers from machine learning and computer vision as well as roboticists to participate.
 
# Contact
See the project website ([berkeleyautomation.github.io/dex-net](berkeleyautomation.github.io/dex-net)) for updates and progress.

For more information please contact [Jeff Mahler](www.jeff-mahler.com) (jmahler@berkeley.edu) or [Prof. Ken Goldberg](http://goldberg.berkeley.edu/) (goldberg@berkeley.edu) of the Berkeley AUTOLAB ([autolab.berkeley.edu](autolab.berkeley.edu)).

# Acknowledgements
This research was performed at the [AUTOLAB](http://autolab.berkeley.edu/) at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab, the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS People and Robots (CPAR) Initiative. The authors were supported in part by the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program, the Berkeley Deep Drive (BDD) Program, and by donations from Siemens, Google, Cisco, Autodesk, IBM, Amazon Robotics, and Toyota Robotics Institute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Sponsors. 

# References
[1] Mahler, Jeffrey, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics." arXiv preprint arXiv:1703.09312 (2017). [(Paper)](https://arxiv.org/abs/1703.09312) [(Website)](http://berkeleyautomation.github.io/dex-net/)

[2] Kappler, Daniel, Jeannette Bohg, and Stefan Schaal. "Leveraging big data for grasp planning." In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 4304-4311. IEEE, 2015.

[3] Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection." arXiv preprint arXiv:1603.02199 (2016).

[4] Johns, Edward, Stefan Leutenegger, and Andrew J. Davison. "Deep learning a grasp function for grasping under gripper pose uncertainty." In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pp. 4461-4468. IEEE, 2016.

[5] Goldfeder, Corey, Matei Ciocarlie, Hao Dang, and Peter K. Allen. "The columbia grasp database." In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pp. 1710-1716. IEEE, 2009.

[6] Prattichizzo, Domenico, and Jeffrey C. Trinkle. "Grasping." In Springer handbook of robotics, pp. 955-988. Springer International Publishing, 2016.

[7] Weisz, Jonathan, and Peter K. Allen. "Pose error robust grasping from contact wrench space metrics." In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 557-562. IEEE, 2012.

[8] Ciocarlie, Matei, Kaijen Hsiao, Edward Gil Jones, Sachin Chitta, Radu Bogdan Rusu, and Ioan A. Şucan. "Towards reliable grasping and manipulation in household environments." In Experimental Robotics, pp. 241-252. Springer Berlin Heidelberg, 2014.

[9] Hernandez, Carlos, Mukunda Bharatheesha, Wilson Ko, Hans Gaiser, Jethro Tan, Kanter van Deurzen, Maarten de Vries et al. "Team Delft's Robot Winner of the Amazon Picking Challenge 2016." arXiv preprint arXiv:1610.05514 (2016).

[10] Mahler, Jeffrey, Florian T. Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kröger, James Kuffner, and Ken Goldberg. "Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards." In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pp. 1957-1964. IEEE, 2016.

[11] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." In Advances in neural information processing systems, pp. 1097-1105. 2012.
