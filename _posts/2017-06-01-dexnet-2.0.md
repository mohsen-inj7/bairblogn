---
layout:     post
title:      "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics"
date:       2017-06-01 9:00:00
author:     Jeff Mahler
visible:    True
excerpt_separator: <!--more-->
---

[![IMAGE ALT TEXT](http://i.imgur.com/BwJ88wo.jpg)](https://www.youtube.com/watch?v=i6K3GI2_EgU "Dex-Net 2.0: 99% Precision Grasping")

Reliable robotic grasping across a wide variety of objects is challenging due to imprecision in sensing and actuation, which leads to uncertainty in object shape, pose, material properties, mass, and the location contact points between the fingers and object. Recent results suggest that deep neural networks trained on large datasets of human grasp labels [2] or physical grasp outcomes [3] can be used to plan grasps that are successful across a wide variety of objects directly from images or point clouds with no explicit modeling of physics, similar to generalization results seen in computer vision. However, current methods require months of execution time on a physical system or tedious human hand-labeling of millions of examples to learn to grasp with up to 90% success.

On the other hand, physics-based models of robust grasping that analyze the distributions of forces and torques that a robot can apply to objects through their fingers [4] can quickly generate millions of grasps automatically using Cloud Computing. These methods typically assume a separate perception system that identifies objects from images and estimates properties such as object pose and shape either perfectly or according to known Gaussian distributions. This is prone to errors, may not generalize well to new objects, and can be slow to match point clouds to known models during execution. Thus, despite over 30 years of research on grasp planning, it is still common to use heuristics in practice for applications such as home decluttering [5] and the Amazon Picking Challenge [6].

<!--more-->

# Dex-Net 2.0: A Hybrid Approach
We propose a hybrid approach that combines the scaling benefits of analytic grasp planning with the generalization ability of empirical approaches based on Deep Learning. Rather than attempt to infer the quantities assumed by analytic grasp planners from images, we use physics-based models of grasping and image formation to construct a probabilistic generative model of virtual point clouds, grasps, and robust grasp metrics from thousands of 3D models [7], and we use samples from the model to train a deep Convolutional Neural Network to plan grasps directly from point clouds.

![alt text](http://i.imgur.com/ChTDZKW.png "Dex-Net 2.0 Grasping Policy")

We formalize and study this approach in our paper, ["Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics,"](https://arxiv.org/abs/1703.09312) which will appear at the Robotics: Science and Systems Conference in July 2017 [1]. In the paper we present the Dexterity Network (Dex-Net) 2.0, a dataset of 6.7 million robust grasps and point clouds with synthetic noise generated from a probabilistic model of grasping rigid objects on a tabletop with a parallel-jaw gripper. We train a deep Grasp Quality Convolutional Neural Network (GQ-CNN) on Dex-Net 2.0 to estimate the probability of success (or *robustness*) of grasps directly from point clouds. We hypothesize that in order to achieve low error on the diversity of data in Dex-Net 2.0, the GQ-CNN must learn to detect generic geometric affordances for grasping such as handles, cylinders, and boxes, [8] that are highly correlated with grasp success. We use the GQ-CNN in a robust grasping policy that finds the grasp with the highest probability of success estimated by the GQ-CNN from a set of antipodal grasp candidates sampled from an input point cloud.

![alt text](http://i.imgur.com/OelLCuA.png "Grasp Quality Convolutional Neural Network")


# Experiments with the ABB YuMi
We evaluated the Dex-Net 2.0 grasping policy with over 1,000 physical trials of grasping single objects on a tabletop with an ABB YuMi robot. To benchmark its performance we compares with several state-of-the-art methods for robotic graspings: image-based grasp heuristics, a grasp detector based on Support Vector Machines, a GQ-CNN trained on only 400 real grasping experiences, and a baseline that recognizes objects, registers their 3D pose, and indexes Dex-Net 1.0 [7] for the most robust grasp to execute.

### Known Objects
In our first experiment we evaluated the performance of grasping known objects on a set of eight objects with adversarial geometry such as smooth, curved surfaces and narrow openings.
Each method was given the exact shape of the objects ahead of time: the Dex-Net 2.0 GQ-CNN was finetuned on the adversarial objects and the registration-based method used an exact CAD model.
We found that the Dex-Net 2.0 grasping policy achieved 93% success and was 3x faster than the registration-based method on a set of known objects with adversarial geometry such as smooth, curved surfaces and narrow openings. This result suggests that the GQ-CNN can be used to plan grasps that are as precise as methods that explicitly use the exact object shape. Here's an example: 

![alt text](https://tctechcrunch2011.files.wordpress.com/2017/05/dex-net.gif "Precision Grasp Planned by Dex-Net 2.0")

### Novel Objects
We also evaluated the ability of each grasp planning method to generalize to previsouly unseen objects. On a set of ten novel household objects never seen in training, Dex-Net 2.0 had the highest success rate of all methods. After analyzing the data though, we found a surprising result: the Dex-Net 2.0 policy predicted zero false positives out of 29 grasps with greater than 50% predicted probability of success. In other words, when the GQ-CNN was confident that a grasp would work, it was always correct.

To further investigate this phenomenon, we ran a follow up experiment with 100 trials of grasping 40 novel objects including objects with articulation, such as a can opener, and deformability, such as a washcloth. The Dex-Net 2.0 policy had 99% precision on these objects, with only one false positive out of 69 grasps predicted to succeed, and succeeded on 94 out of 100 grasps. This suggests that the policy could be used in applications requiring high reliability such as warehousing and manufacturing.

# Discussion and Future Work
Overall, the results of grasp planning with Dex-Net 2.0 suggest that it is possible to achieve highly reliable grasping across a wide variety of objects by training neural networks with only synthetic data generated using physical models of grasping and image formation. The grasping policy transfers directly to the physical robot without data collection on the physical system or complex transfer learning technques, and we hope this will facilitate its use in new research and industrial applications.

In future work we plan to extend our method to manipulation tasks with object interctions and sequential structure. We've started experimenting with decluttering, where the goal is to grasp objects from an unstructured pile on a tabletop and move them to a bin. Interestingly, we found the the Dex-net 2.0 policy achieves over 90% (one false positive out of every 10 predictions) when grasping objects in a pile with no further modifications. This surprised us since the GQ-CNN was trained on only objects in isolation, and the result suggests that the GQ-CNN learned to detect generic grasp affordances as we hypothesized earlier in the post. We're also experimenting with re-grasping: controlling the 3D orientation of objects for assembly, packing objects tightly into boxes, or handing off to another arm.

# Dataset and Code Release
We also plan to release a subset of our code, dataset, and the trained GQ-CNN weights over summer 2017 to facilitate further research and comparisons. We are aiming for the following tentative release dates:
* **Dex-Net 2.0 GQ-CNN Training Dataset and ROS Package:** June 19, 2017.
* **Dex-Net 2.0 Database with Python API:** July 11, 2017.
* **Full Dex-Net 2.0 Python and Web API with Functionality to Compute Grasps and Upload Models:** September 1, 2017.

Stay tuned and please contact [Jeff Mahler](www.jeff-mahler.com) (jmahler@berkeley.edu) or [Prof. Ken Goldberg](http://goldberg.berkeley.edu/) (goldberg@berkeley.edu) with questions or specific applications of interest.

# Acknowledgements
This research was performed at the [AUTOLAB](http://autolab.berkeley.edu/) at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab, the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS People and Robots (CPAR) Initiative. The authors were supported in part by the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program, the Berkeley Deep Drive (BDD) Program, and by donations from Siemens, Google, Cisco, Autodesk, IBM, Amazon Robotics, and Toyota Robotics Institute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Sponsors. 

# References
[1] Mahler, Jeffrey, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics." arXiv preprint arXiv:1703.09312 (2017). [Paper](https://arxiv.org/abs/1703.09312) [Website](http://berkeleyautomation.github.io/dex-net/)

[2] Kappler, Daniel, Jeannette Bohg, and Stefan Schaal. "Leveraging big data for grasp planning." In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 4304-4311. IEEE, 2015.

[3] Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection." arXiv preprint arXiv:1603.02199 (2016).

[4] Prattichizzo, Domenico, and Jeffrey C. Trinkle. "Grasping." In Springer handbook of robotics, pp. 955-988. Springer International Publishing, 2016.

[5] Ciocarlie, Matei, Kaijen Hsiao, Edward Gil Jones, Sachin Chitta, Radu Bogdan Rusu, and Ioan A. Şucan. "Towards reliable grasping and manipulation in household environments." In Experimental Robotics, pp. 241-252. Springer Berlin Heidelberg, 2014.

[6] Hernandez, Carlos, Mukunda Bharatheesha, Wilson Ko, Hans Gaiser, Jethro Tan, Kanter van Deurzen, Maarten de Vries et al. "Team Delft's Robot Winner of the Amazon Picking Challenge 2016." arXiv preprint arXiv:1610.05514 (2016).

[7] Mahler, Jeffrey, Florian T. Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kröger, James Kuffner, and Ken Goldberg. "Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards." In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pp. 1957-1964. IEEE, 2016.

[8] Detry, Renaud, Emre Baseski, Mila Popovic, Younes Touati, N. Kruger, Oliver Kroemer, Jan Peters, and Justus Piater. "Learning object-specific grasp affordance densities." In Development and Learning, 2009. ICDL 2009. IEEE 8th International Conference on, pp. 1-7. IEEE, 2009.
