---
layout:             post
title:              "Model-Based Reinforcement Learning: Theory and Practice"
date:               2019-12-12 9:00:00
author:             <a href="https://people.eecs.berkeley.edu/~janner/">Michael Janner</a>
img:                assets/mbpo/teaser-01.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---


<article class="post-content">

<!-- begin section I: introduction -->

<p>
    Reinforcement learning systems can make decisions in one of two ways. In the <i>model-based</i> approach, a system uses a predictive model of the world to ask questions of the form “what will happen if I do <i>x</i>?” to choose the best <i>x</i><sup id="fnref:naming-conventions"><a href="#fn:naming-conventions" class="footnote"><font size="-2">1</font></a></sup>. In the alternative <i>model-free</i> approach, the modeling step is bypassed altogether in favor of learning a control policy directly. Although in practice the line between these two techniques can become blurred, as a coarse guide it is useful for dividing up the space of algorithmic possibilities.
</p>

<p style="text-align:center;">
    <img src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/teaser-01.png" width="75%">
    <br>
    <i>Predictive models can be used to ask “what if?” questions to guide future decisions.</i>
</p>

<p>
    The natural question to ask after making this distinction is whether to use such a predictive model. The field has grappled with <a href="https://www.cs.cmu.edu/~tom/10701_sp11/slides/Kaelbling.pdf#page=15">this question</a> for quite a while, and is unlikely to reach a consensus any time soon. However, we have learned enough about designing model-based algorithms that it is possible to draw some general conclusions about best practices and common pitfalls. In this post, we will survey various realizations of model-based reinforcement learning methods. We will then describe some of the tradeoffs that come into play when using a learned predictive model for training a policy and how these considerations motivate a simple but effective strategy for model-based reinforcement learning. The latter half of this post is based on our recent paper on <a href="https://arxiv.org/abs/1906.08253">model-based policy optimization</a>, for which code is available <a href="https://github.com/JannerM/mbpo">here</a>.
</p>

<!--more-->

<!-- begin section II: model-based techniques -->

<h2 id="model-based-techniques">Model-based techniques</h2>

<p>
    Below, model-based algorithms are grouped into four categories to highlight the range of uses of predictive models. For the comparative performance of some of these approaches in a continuous control setting, this <a href="https://arxiv.org/abs/1907.02057">benchmarking paper</a> is highly recommended.
</p>

<p>
    <strong>Analytic gradient computation</strong>
</p>

<p>
    Assumptions about the form of the dynamics and cost function are convenient because they can yield closed-form solutions for locally optimal control, as in the <a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/slides/Lec5-LQR.pdf#page=11">LQR framework</a>. Even when these assumptions are not valid, <a href="https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf">receding</a>-<a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">horizon</a> <a href="http://www.youtube.com/watch?v=anIsw2-Lbco&t=3m5s">control</a> can account for small errors introduced by approximated dynamics. Similarly, dynamics models parametrized as <a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">Gaussian processes</a> have analytic gradients that can be used for policy improvement. Controllers derived via these simple parametrizations can also be used to provide <a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf">guiding samples</a> for training more complex nonlinear policies.
</p>

<p>
    <strong>Sampling-based planning</strong>
</p>

<p>
    In the fully general case of nonlinear dynamics models, we lose guarantees of local optimality and must resort to sampling action sequences. The simplest version of this approach, <a href="https://arxiv.org/abs/1708.02596">random shooting</a>, entails sampling candidate actions from a fixed distribution, evaluating them under a model, and choosing the action that is deemed the most promising. More sophisticated variants iteratively adjust the sampling distribution, as in the <a href="https://www.sciencedirect.com/science/article/pii/B9780444538598000035">cross-entropy method</a> (CEM; used in <a href="https://arxiv.org/abs/1811.04551">PlaNet</a>, <a href="https://arxiv.org/abs/1805.12114">PETS</a>, and <a href="https://arxiv.org/abs/1610.00696">visual</a> <a href="https://arxiv.org/abs/1812.00568">foresight</a>) or <a href="https://arxiv.org/abs/1509.01149">path integral optimal control</a> (used in recent model-based <a href="https://arxiv.org/abs/1909.11652">dexterous manipulation</a> work).
</p>

<p>
    In discrete-action settings, however, it is more common to search over tree structures than to iteratively refine a single trajectory of waypoints. Common tree-based search algorithms include <a href="https://hal.inria.fr/inria-00116992/document">MCTS</a>, which has underpinned recent impressive results in <a href="https://arxiv.org/abs/1712.01815">games</a> <a href="https://arxiv.org/abs/1705.08439">playing</a>, and <a href="https://www.ijcai.org/Proceedings/15/Papers/230.pdf">iterated width search</a>. Sampling-based planning, in both continuous and discrete domains, can also be combined with <a href="https://arxiv.org/abs/1904.03177">structured</a> <a href="https://arxiv.org/abs/1907.09620">physics-based</a>, <a href="https://arxiv.org/abs/1910.12827">object-centric</a> priors.
</p>

<p>
    <strong>Model-based data generation</strong>
</p>

<p>
    An important detail in many <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf#page=5">machine learning success stories</a> is a means of artificially increasing the size of a training set. It is difficult to define a manual data augmentation procedure for policy optimization, but we can view a predictive model analogously as a learned method of generating synthetic data. The original proposal of such a combination comes from the <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.7362&rep=rep1&type=pdf">Dyna algorithm</a> by Sutton, which alternates between model learning, data generation under a model, and policy learning using the model data. This strategy has been combined with <a href="https://arxiv.org/abs/1603.00748">iLQG</a>, <a href="https://arxiv.org/abs/1802.10592">model ensembles</a>, and <a href="https://arxiv.org/abs/1809.05214">meta-learning</a>; has been scaled <a href="https://arxiv.org/abs/1506.07365">to</a> <a href="https://arxiv.org/abs/1803.10122">image</a> <a href="https://arxiv.org/abs/1903.00374">observations</a>; and is amenable to <a href="https://arxiv.org/abs/1807.03858">theoretical analysis</a>. A close cousin to model-based data generation is the use of a model to improve <a href="https://arxiv.org/abs/1803.00101">target</a> <a href="https://arxiv.org/abs/1807.01675">value</a> estimates for temporal difference learning.
</p>

<p>
    <strong>Value-equivalence prediction</strong>
</p>

<p>
    A final technique, which does not fit neatly into model-based versus model-free categorization, is to incorporate computation that resembles <a href="https://arxiv.org/abs/1810.13400">model-based planning</a> without supervising the model’s predictions to resemble actual states. Instead, plans under the model are constrained to match trajectories in the real environment only in their predicted cumulative reward. These <a href="https://arxiv.org/abs/1602.02867">value-equivalent models</a> have shown to be effective in high-dimensional <a href="https://arxiv.org/abs/1707.03497">observation</a> <a href="https://arxiv.org/abs/1911.08265">spaces</a> where conventional model-based planning has proven difficult.
</p>

<!-- begin section III: trade-offs -->

<h2 id="trade-offs-of-model-data">Trade-offs of model data</h2>

<p>
    In what follows, we will focus on the data generation strategy for model-based reinforcement learning. It is not obvious whether incorporating model-generated data into an otherwise model-free algorithm is a good idea. Modeling errors could cause <a href="https://arxiv.org/abs/1906.05243">diverging temporal-difference updates</a>, and in the case of linear approximation, <a href="https://users.cs.duke.edu/~parr/icml08.pdf">model and value fitting are equivalent</a>. However, it is easier to motivate model usage by considering the empirical generalization capacity of predictive models, and such a model-based augmentation procedure turns out to be surprisingly effective in practice.
</p>

<p>
    <strong>The Good News</strong>
</p>

<p>
    A natural way of thinking about the effects of model-generated data begins with the standard objective of reinforcement learning:
</p>

<p style="text-align:center;">
    <img src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/objective.png" width="50%">
</p>

<p>
    which says that we want to maximize the expected cumulative discounted rewards \(r(s_t, a_t)\) from acting according to a policy \(\pi\) in an environment governed by dynamics \(p\). It is important to pay particular attention to the distributions over which this expectation is taken.<sup id="fnref:initial-distribution"><a href="#fn:initial-distribution" class="footnote"><font size="-2">2</font></a></sup> For example, while the expectation is supposed to be taken over trajectories from the current policy \(\pi\), in practice many algorithms re-use trajectories from an old policy \(\pi_\text{old}\) for improved sample-efficiency. There has been much <a href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&context=cs_faculty_pubs">algorithm</a> <a href="https://arxiv.org/abs/1606.02647">development</a> dedicated to correcting for the issues associated with the resulting <i>off-policy error</i>.
</p>

<p>
    Using model-generated data can also be viewed as a simple modification of the sampling distribution. Incorporating model data into policy optimization amounts to swapping out the true dynamics \(p\) with an approximation \(\hat{p}\). The <i>model bias</i> introduced by making this substitution acts analogously to the off-policy error, but it allows us to do something rather useful: we can query the model dynamics \(\hat{p}\) at any state to generate samples from the current policy, effectively circumventing the off-policy error.
</p>

<p>
    If model usage can be viewed as trading  between off-policy error and model bias, then a straightforward way to proceed would be to compare these two terms. However, estimating a model’s error on the <i>current</i> policy’s distribution requires us to make a statement about how that model will generalize. While worst-case bounds are rather pessimistic here, we found that predictive models tend to generalize to the state distributions of future policies well enough to motivate their usage in policy optimization.
</p>

<p style="text-align:center;">
    <img src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/generalization.png" width="75%">
    <br>
    <i>Generalization of learned models, trained on samples from a data-collecting policy \(\pi_D\) , to the state distributions of future policies \(\pi\) seen during policy optimization. Increasing the training set size not only improves performance on the training distribution, but also on nearby distributions.</i>
</p>


<p>
    <strong>The Bad News</strong>
</p>

<p>
    The above result suggests that the single-step predictive accuracy of a learned model can be reliable under policy shift. The catch is that most model-based algorithms rely on models for much more than single-step accuracy, often performing model-based rollouts equal in length to the task horizon in order to properly estimate the state distribution under the model. When predictions are strung together in this manner, small <a href="https://arxiv.org/abs/1905.13320">errors</a> <a href="https://arxiv.org/abs/1612.06018">compound</a> over the prediction horizon.
</p>

<p style="text-align:center;">
    <img src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/mbpo_hopper_loop.gif" width="100%">
    <br>
    <i>A 450-step action sequence rolled out under a learned probabilistic model, with the figure’s position depicting the mean prediction and the shaded regions corresponding to one standard deviation away from the mean. The growing uncertainty and deterioration of a recognizable sinusoidal motion underscore accumulation of model errors.</i>
</p>

<!-- <video width=100% height=auto autoplay playsinline muted>
  <source src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/mbpo_hopper_loop.gif" type="video/mp4">
</video> -->

<p>
    <strong>Analyzing the trade-off</strong>
</p>

<p>
    This qualitative trade-off can be made more precise by writing a lower bound on a policy’s true return in terms of its model-estimated return:
</p>

<p style="text-align:center;">
    <img src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/bound.png" width="75%">
    <br>
    <i>A lower bound on a policy’s
        <span style="background-color: rgba(104,204,255,.5)">true return</span> in terms of its expected <span style="background-color: rgba(255,199,70,.5)">model return</span>, the <span style="background-color: rgba(255,109,110,.5)">model rollout length</span>, the <span style="background-color: rgba(106,225,106,.5)">policy divergence</span>, and the <span style="background-color: rgba(242,126,48,.5)">model error</span> on the current policy’s state distribution.</i>
</p>

<p>
    As expected, there is a tension involving the model rollout length. The model serves to reduce off-policy error via the terms exponentially decreasing in the rollout length \(k\). However, increasing the rollout length also brings about increased discrepancy proportional to the model error.
</p>

<!-- begin section IV: MBPO -->

<h2 id="model-based-policy-optimization">Model-based policy optimization</h2>

<p>
    We have two main conclusions from the above results:
</p>

<ol>
    <li>predictive models can generalize well enough for the incurred model bias to be worth the reduction in off-policy error, but</li>
    <li>compounding errors make long-horizon model rollouts unreliable.</li>
</ol>

<p>
    A simple recipe for combining these two insights is to use the model only to perform short rollouts from all previously encountered real states instead of full-length rollouts from the initial state distribution. Variants of this procedure have been studied in prior works dating back to the classic <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.7362&rep=rep1&type=pdf">Dyna algorithm</a>, and we will refer to it generically as model-based policy optimization (MBPO), which we summarize in the pseudo-code below.
</p>

<p style="text-align:center;">
    <img src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/pseudo_code.png" width="75%">
</p>

<p>
    We found that this simple procedure, combined with a few important design decisions like using <a href="https://arxiv.org/abs/1805.12114">probabilistic model ensembles</a> and a <a href="https://arxiv.org/abs/1801.01290">stable off-policy model-free optimizer</a>, yields the best combination of sample efficiency and asymptotic performance. We also found that MBPO avoids the pitfalls that have prevented recent model-based methods from scaling to higher-dimensional states and long-horizon tasks.
</p>

<p style="text-align:center;">
    <img src="https://people.eecs.berkeley.edu/~janner/mbpo/blog/figures/consolidated.png" width="75%">
    <br>
    <i>Learning curves of MBPO and five prior works on continuous control benchmarks. MBPO reaches the same asymptotic performance as the best model-free algorithms, often with only one-tenth of the data, and scales to state dimensions and horizon lengths that cause previous model-based algorithms to fail.</i>
</p>

<hr>

<p>
    This post is based on the following paper:
</p>

<ul>
    <li>
        <a href="https://arxiv.org/abs/1906.08253"><strong>When to Trust Your Model: Model-Based Policy Optimization</strong></a>
        <br>
        <a href="https://people.eecs.berkeley.edu/~janner/">Michael Janner</a>, <a href="https://people.eecs.berkeley.edu/~justinjfu/">Justin Fu</a>, <a href="http://marvinzhang.com/">Marvin Zhang</a>, and <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
        <br>
        <em>Neural Information Processing Systems (NeurIPS), 2019.</em>
        <br>
        <a href="https://github.com/JannerM/mbpo">Open-source code</a>
    </li>
</ul>

<p>
    <em>I would like to thank Michael Chang and Sergey Levine for their valuable feedback.</em>
</p>

<hr>

<div class="footnotes">
  <ol>
    <li id="fn:naming-conventions">
      <p>
         In reinforcement learning, this variable is typically denoted by <i>a</i> for “action.” In control theory, it is denoted by <i>u</i> for “upravleniye” (or more faithfully, “управление”), which I am told is “control” in Russian.<a href="#fnref:naming-conventions" class="reversefootnote">↩</a>
     </p>
    </li>
    <li id="fn:initial-distribution">
      <p>
        We have omitted the initial state distribution \(s_0 \sim \rho(\cdot)\) to focus on those distributions affected by incorporating a learned model.<a href="#fnref:initial-distribution" class="reversefootnote">↩</a>
    </p>
    </li>
  </ol>
</div>

<hr>

<p>
<font size="-1">
<strong>References</strong>
<ol>
    <li>KR Allen, KA Smith, and JB Tenenbaum. <a href="https://arxiv.org/abs/1907.09620">The tools challenge: rapid trial-and-error learning in physical problem solving.</a> CogSci 2019.</li>
    <li>B Amos, IDJ Rodriguez, J Sacks, B Boots, JZ Kolter. <a href="https://arxiv.org/abs/1810.13400">Differentiable MPC for end-to-end planning and control.</a> NeurIPS 2018.</li>
    <li>T Anthony, Z Tian, and D Barber. <a href="https://arxiv.org/abs/1705.08439">Thinking fast and slow with deep learning and tree search.</a> NIPS 2017.</li>
    <li>K Asadi, D Misra, S Kim, and ML Littman. <a href="https://arxiv.org/abs/1905.13320">Combating the compounding-error problem with a multi-step model.</a> arXiv 2019.</li>
    <li>V Bapst, A Sanchez-Gonzalez, C Doersch, KL Stachenfeld, P Kohli., PW Battaglia, and JB Hamrick. <a href="https://arxiv.org/abs/1904.03177">Structured agents for physical construction.</a> ICML 2019.</li>
    <li>ZI Botev,  DP Kroese,  RY Rubinstein,  and P L’Ecuyer.   <a href="https://www.sciencedirect.com/science/article/pii/B9780444538598000035">The cross-entropy method for optimization.</a> Handbook of Statistics, volume 31, chapter 3. 2013.</li>
    <li>J Buckman, D Hafner, G Tucker, E Brevdo, and H Lee. <a href="https://arxiv.org/abs/1807.01675">Sample-efficient reinforcement learning with stochastic ensemble value expansion.</a> NeurIPS 2018.</li>
    <li>K Chua, R Calandra, R McAllister, and S Levine. <a href="https://arxiv.org/abs/1805.12114">Deep reinforcement learning in a handful of trials using probabilistic dynamics models.</a> NeurIPS 2018.</li>
    <li>I Clavera, J Rothfuss, J Schulman, Y Fujita, T Asfour, and P Abbeel. <a href="https://arxiv.org/abs/1809.05214">Model-based reinforcement learning via meta-policy optimization.</a> CoRL 2018.</li>
    <li>R Coulom. <a href="https://hal.inria.fr/inria-00116992/document">Efficient selectivity and backup operators in Monte-Carlo tree search.</a> CG 2006.</li>
    <li>M Deisenroth and CE Rasmussen. <a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">PILCO: A model-based and data-efficient approach to policy search.</a> ICML 2011.</li>
    <li>F Ebert, C Finn, S Dasari, A Xie, A Lee, and S Levine. <a href="https://arxiv.org/abs/1812.00568">Visual foresight: model-based deep reinforcement learning for vision-based robotic control.</a> arXiv 2018.</li>
    <li>V Feinberg, A Wan, I Stoica, MI Jordan, JE Gonzalez, and S Levine.  <a href="https://arxiv.org/abs/1803.00101">Model-based value estimation for efficient model-free reinforcement learning.</a> ICML 2018.</li>
    <li>C Finn and S Levine.  <a href="https://arxiv.org/abs/1610.00696">Deep visual foresight for planning robot motion.</a> ICRA 2017.</li>
    <li>S Gu, T Lillicrap, I Sutskever, and S Levine.  <a href="https://arxiv.org/abs/1603.00748">Continuous deep Q-learning with model-based acceleration.</a> ICML 2016.</li>
    <li>D Ha and J Schmidhuber.  <a href="https://arxiv.org/abs/1803.10122">World models.</a> NeurIPS 2018.</li>
    <li>T Haarnoja, A Zhou, P Abbeel, and S Levine.  <a href="https://arxiv.org/abs/1801.01290">Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor.</a> ICML 2018.</li>
    <li>D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, and J Davidson.  <a href="https://arxiv.org/abs/1811.04551">Learning latent dynamics for planning from pixels.</a> ICML 2019.</li>
    <li>LP Kaelbling, ML Littman, and AP Moore. <a href="https://www.cs.cmu.edu/~tom/10701_sp11/slides/Kaelbling.pdf#page=15">Reinforcement learning: a survey.</a> JAIR 1996.</li>
    <li>L Kaiser, M Babaeizadeh, P Milos, B Osinski, RH Campbell, K Czechowski, D Erhan, C Finn, P Kozakowsi, S Levine, R Sepassi, G Tucker, and H Michalewski. <a href="https://arxiv.org/abs/1903.00374">Model-based reinforcement learning for Atari.</a> arXiv 2019.</li>
    <li>A Krizhevsky, I Sutskever, and GE Hinton.  <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf#page=5">ImageNet classification with deep convolutional neural networks.</a> NIPS 2012.</li>
    <li>T Kurutach, I Clavera, Y Duan, A Tamar, and P Abbeel. <a href="https://arxiv.org/abs/1802.10592">Model-ensemble trust-region policy optimization.</a> ICLR 2018.</li>
    <li>S Levine and V Koltun. <a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf">Guided policy search.</a> ICML 2013.</li>
    <li>W Li and E Todorov.  <a href="https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf">Iterative linear quadratic regulator design for nonlinear biological movement systems.</a> ICINCO 2004.</li>
    <li>N Lipovetzky, M Ramirez, and H Geffner. <a href="https://www.ijcai.org/Proceedings/15/Papers/230.pdf">Classical planning with simulators: results on the Atari video games.</a> IJCAI 2015.</li>
    <li>Y Luo, H Xu, Y Li, Y Tian, T Darrell, and T Ma. <a href="https://arxiv.org/abs/1807.03858">Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees.</a> ICLR 2019.</li>
    <li>R Munos, T Stepleton, A Harutyunyan, MG Bellemare. <a href="https://arxiv.org/abs/1606.02647">Safe and efficient off-policy reinforcement learning.</a> NIPS 2016.</li>
    <li>A Nagabandi, K Konoglie, S Levine, and V Kumar. <a href="https://arxiv.org/abs/1909.11652">Deep dynamics models for learning dexterous manipulation.</a> arXiv 2019.</li>
    <li>A Nagabandi, GS Kahn, R Fearing, and S Levine. <a href="https://arxiv.org/abs/1708.02596">Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning.</a> ICRA 2018.</li>
    <li>J Oh, S Singh, and H Lee. <a href="https://arxiv.org/abs/1707.03497">Value prediction network.</a> NIPS 2017.</li>
    <li>R Parr, L Li, G Taylor, C Painter-Wakefield, ML Littman. <a href="https://users.cs.duke.edu/~parr/icml08.pdf">An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning.</a> ICML 2008.</li>
    <li>D Precup, R Sutton, and S Singh. <a href="http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&context=cs_faculty_pubs">Eligibility traces for off-policy policy evaluation.</a> ICML 2000.</li>
    <li>J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, T Lillicrap, and D Silver. <a href="https://arxiv.org/abs/1911.08265">Mastering Atari, Go, chess and shogi by planning with a learned model.</a> arXiv 2019.</li>
    <li>D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Ku-maran, T Graepel, TP Lillicrap, K Simonyan, and D Hassabis. <a href="https://arxiv.org/abs/1712.01815">Mastering chess and shogi by self-play with a general reinforcement learning algorithm.</a> arXiv 2017.</li>
    <li>RS Sutton. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.7362&rep=rep1&type=pdf">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming.</a> ICML 1990.</li>
    <li>E Talvitie. <a href="https://arxiv.org/abs/1612.06018">Self-correcting models for model-based reinforcement learning.</a> AAAI 2016.</li>
    <li>A Tamar, Y Wu, G Thomas, S Levine, and P Abbeel. <a href=https://arxiv.org/abs/1602.02867>Value iteration networks.</a> NIPS 2016.</li>
    <li>Y Tassa, T Erez, and E Todorov. <a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">Synthesis and stabilization of complex behaviors through online trajectory optimization.</a> IROS 2012.</li>
    <li>H van Hasselt, M Hessel, and J Aslanides.  <a href="https://arxiv.org/abs/1906.05243">When to use parametric models in reinforcement learning?</a> NeurIPS 2019.</li>
    <li>R Veerapaneni, JD Co-Reyes, M Chang, M Janner, C Finn, J Wu, JB Tenenbaum, and S Levine.  <a href="https://arxiv.org/abs/1910.12827">Entity abstraction in visual model-based reinforcement learning.</a> CoRL 2019.</li>
    <li>T Wang, X Bao, I Clavera, J Hoang, Y Wen, E Langlois, S Zhang, G Zhang, P Abbeel, and J Ba. <a href="https://arxiv.org/abs/1907.02057">Benchmarking model-based reinforcement learning.</a> arXiv 2019.</li>
    <li>M Watter, JT Springenberg, J Boedecker, M Riedmiller. <a href="https://arxiv.org/abs/1506.07365">Embed to control: a locally linear latent dynamics model for control from raw images.</a> NIPS 2015.</li>
    <li>G Williams, A Aldrich, and E Theodorou. <a href="https://arxiv.org/abs/1509.01149">Model predictive path integral control using covariance variable importance sampling.</a> arXiv 2015.</li>
</ol>
</font>
</p>


</article>
