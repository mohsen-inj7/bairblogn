---
layout:     post
title:      "Constrained Policy Optimization"
date:       2017-07-04 10:00:00
author:     Joshua Achiam
visible:    True
excerpt_separator: <!--more-->
---
(Based on joint work with David Held, Aviv Tamar, and Pieter Abbeel.)

Deep reinforcement learning (RL) has enabled some remarkable achievements in hard control problems: with deep RL, agents have learned to [play video games directly from pixels](https://arxiv.org/abs/1602.01783), to control robots to [interact with objects from demonstration](https://blog.openai.com/robots-that-learn/), and even to [beat human grandmasters at Go](https://deepmind.com/research/publications/mastering-game-go-deep-neural-networks-tree-search/). Hopefully, we’ll soon be able to take deep RL out of the lab and put it into practical, everyday technologies, like UAV control and household robots. But before we can do that, we have to address the most important concern: safety.

We recently developed a principled way to incorporate safety requirements and other constraints directly into a family of state-of-the-art deep RL algorithms. Our approach, Constrained Policy Optimization (CPO), makes sure that the agent satisfies constraints at every step of the learning process. Specifically, we try to satisfy constraints on _costs_: the designer assigns a cost and a limit for each outcome that the agent should avoid, and the agent learns to keep all of its costs below their limits. This kind of [constrained RL](http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf) approach has been around for a long time, but CPO is the first algorithm that makes it practical to combine constrained RL with deep learning. (Plus, CPO has nice theoretical guarantees!)


[In our paper](https://arxiv.org/abs/1705.10528), we describe an efficient way to run CPO, and we show that CPO can train neural network agents to maximize rewards while satisfying constraints in several tasks involving realistic robot simulations. If you want to try it out, [we've also open-sourced our code!](https://github.com/jachiam/cpo)


<!--more-->

# Why Do We Need Constraints for Safety?


RL agents are trained to maximize a reward signal, which must be specified in advance by a human designer. If the reward signal isn’t properly designed, the agent [can learn unintended or even harmful behavior](https://arxiv.org/abs/1606.06565). If it were easy to design reward functions, this wouldn't be an issue, but unfortunately it's fundamentally challenging; this is a key motivation for using constraints. 

To illustrate, let’s consider a simplified example based on a real-world use case: an autonomous car which is supposed to drive to its destination and avoid collisions. We'll require that the collision frequency for the car is less than some pre-selected threshold, for the sake of safety. If the frequency of collisions is below that threshold, we’ll consider the car “safe,” and otherwise, it’s “unsafe.”<!--loosely on a real-world use case: an autonomous robot which is supposed to perform some kind of task while not leaving a safe area. We'll require that the frequency with which it leaves the safe area is less than some pre-selected threshold, for the sake of safety. If the frequency of collisions is below that threshold, we’ll consider the car “safe,” and otherwise, it’s “unsafe.” --> 

This kind of problem is easy to describe in the constrained RL setting, in terms of both rewards (for getting to the destination) and constraints (on collision frequency). But for standard RL, it's quite challenging, because we have to design all behavior through the reward function alone. Mistakes in reward design could result in agents that are either too risk-averse, and therefore useless, or too risk-prone, and therefore dangerous. 

Suppose that the car gets a reward of +1 for getting to its destination, and a reward of -X for being involved in a collision. After training, the policy will still get into collisions sometimes, but the frequency of collisions will depend on X. If X is larger, the frequency of collisions will be smaller, and vice versa. But this intuition is not *rigorous*: we have no formula for predicting collision frequency from X, and as a result, we cannot pick X in advance to attain a design specification on safety. So in order to learn a safe policy with standard RL, we would have to train many different policies with different values of X, until we find one that works. 

(Show Ant-Circle for X=1, X=5?)

This is bad because of the dynamics of the learning process. RL agents learn by trial and error, and they *explore* by trying many different policies before converging. Although sometimes high-fidelity simulators of the real world can help accelerate training, RL agents for the real world ultimately have to do *some* of their exploration in the real world. This means that if we pick X wrong, we could put unsafe policies in real driving situations—which would clearly be unacceptable! Importantly, even choices of X that lead to safe policies at optimum could result in unsafe exploration somewhere between the beginning and end of training. 

In many cases in the real world, like in the example, it’s easier to specify constraints than choose tradeoffs: a system is often considered safe if it behaves predictably, in the sense that failure modes are rare and happen less often than some known chosen at design time. This motivates constrained RL as the natural way to incorporate safety into RL. Furthermore, from thinking about the learning dynamics, we want to make sure that *every exploration policy* is also constraint-satisfying. 

# CPO: Local Policy Search for Constrained RL

A standard way to learn policies is by local policy search, where we iteratively improve policies until convergence to optimum. This kind of search is called 'local' because each new policy is required to be close, in some sense, to the old one. To give an example, policy gradient methods are local policy search algorithms which keep the policy parameters close in $$L_2$$-norm. (If you're unfamiliar with policy gradients, [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/) has a nice introduction!)

[Trust region methods](https://arxiv.org/abs/1502.05477) are another kind of local policy search algorithm. They also use policy gradients, but they make a special requirement for how policies are updated: each new policy has to be close to the old one in terms of **average [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)**. Why should we do this, instead of an $$L_2$$ norm constraint on policy parameters? Well, KL-divergence is like a distance measure between policies in *policy space,* instead of a distance measure in *parameter space.* It turns out that a small change in parameters can result in a big change in the policy, and gradient-methods don't work well with very large steps. 

(Picture showing how a L2-ball in parameter space maps to a weird-shaped ball in policy space?)

Consequently, trust region methods wind up performing extremely well in practice for training neural network policies, and are able to avoid the issue of performance collapse that standard policy gradient algorithms can sometimes suffer from (see [Duan et al., 2016](https://arxiv.org/abs/1604.06778)). 

CPO is a trust region method for constrained RL which approximately enforces the constraints in every policy update. It uses approximations of the constraints to predict how much the constraint costs might change after any given update, and then chooses the update that will most improve performance while keeping the constraint costs below their limits. 

We also have some nice theoretical results to complement the practical algorithm: we derive a new bound to describe the quality of our approximations, in terms of the average KL-divergence. This lets us prove a guarantee on the performance of trust region methods in general, which explains how well they do, and also lets us guarantee the worst-case constraint violation which is possible after a CPO update.

(Example of empirical results: point-gather + gif?)


# Conclusions

In this work, we described a method for doing reinforcement learning with constraints which is theoretically well-founded and empirically successful. Our method, CPO, represents a step towards bringing RL out of the lab and into the real world, by enabling us to explicitly specify safety requirements when training RL agents. 


